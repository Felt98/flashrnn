{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"1\"\n",
    "import torch\n",
    "\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_printoptions(linewidth=300, threshold=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from flashrnn.flashrnn import flashrnn\n",
    "\n",
    "from flashrnn.flashrnn.vanilla_fwbw.fw import forward_sequence, lstm_pointwise_fw\n",
    "from flashrnn.flashrnn.vanilla_fwbw.fwbw import lstm_pt_fwbw\n",
    "from flashrnn.flashrnn.triton_fused.fwbw import lstm_tr_fwbw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match LSTM triton kernel to torch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "TGT_DTYPE = torch.float32\n",
    "B = 3  # batch size\n",
    "T = 23  # sequence length\n",
    "NG = 4  # number of gates (NGI == NGR)\n",
    "NH = 5  # number of heads\n",
    "D = 32  # input/hidden (embedding) dimension\n",
    "NS = 2  # number of states (c, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# Wx = torch.zeros([B, T, NG, NH, D], device=device, dtype=dtype)\n",
    "# Wx[:, :, 0, :, :] = 1.0 # input gate\n",
    "# Wx[:, :, 1, :, :] = 2.0 # forget gate\n",
    "# Wx[:, :, 2, :, :] = 3.0 # cell gate\n",
    "# # Wx[1, 2, 2, :, 5] = 500.\n",
    "# Wx[:, :, 3, :, :] = 4.0 # output gate\n",
    "# R = torch.zeros([NG, NH, D, D], device=device, dtype=dtype)\n",
    "# R[0, :, :, :] = 1.0 # input gate\n",
    "# R[1, :, :, :] = 2.0 # forget gate\n",
    "# R[2, :, :, :] = 3.0 # cell gate\n",
    "# R[2, :, 1, 1] = 1.11\n",
    "# R[3, :, :, :] = 4.0 # output gate\n",
    "# b = torch.zeros([NG, NH, D], device=device, dtype=dtype)\n",
    "# b[0, :, :] = 1.0\n",
    "# b[1, :, :] = 2.0\n",
    "# b[2, :, :] = 3.0\n",
    "# b[3, :, :] = 4.0\n",
    "# states_initial = torch.zeros([NS, B, NH, D], device=device, dtype=dtype)\n",
    "# states_initial[0, :, :, :] = 1.0\n",
    "# states_initial[0, 0, :, 1] = 50.0\n",
    "# states_initial[1, :, :, :] = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "Wx = torch.randn([B, T, NG, NH, D], device=device, dtype=dtype)\n",
    "R = torch.randn([NG, NH, D, D], device=device, dtype=dtype) / (D**0.5)\n",
    "b = torch.randn([NG, NH, D], device=device, dtype=dtype)\n",
    "states_initial = torch.zeros([NS, B, NH, D], device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Direct Function Call] Check for numerical correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# •\tWx.clone()：创建 Wx 的一个副本，确保后续操作不会影响原始张量。\n",
    "\t# •\t.to(TGT_DTYPE)：将张量转换为目标数据类型（如 torch.float32 或 torch.float16）。\n",
    "\t# •\t.detach()：从当前的计算图中分离张量，防止其历史操作被追踪。\n",
    "\t# •\t.requires_grad_(True)：启用梯度计算\n",
    "\n",
    "Wx_mpt_ag = Wx.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "R_mpt_ag = R.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "b_mpt_ag = b.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "states_initial_mpt_ag = (\n",
    "    states_initial.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行 flashrnn中LSTM 的并行点式前向传播\n",
    "\t# •\th_mpt_ag：整个序列的隐藏状态输出，形状为 (sequence_length, batch_size, hidden_dim)。\n",
    "\t# •\thlast_mpt_ag：序列的最后一个隐藏状态，形状为 (batch_size, hidden_dim)。\n",
    "h_mpt_ag, hlast_mpt_ag = forward_sequence(\n",
    "    states_initial=states_initial_mpt_ag,\n",
    "    Wx=Wx_mpt_ag,\n",
    "    R=R_mpt_ag,\n",
    "    b=b_mpt_ag,\n",
    "    forward_pointwise=lstm_pointwise_fw,\n",
    "    output_gates_and_states_initial=False,\n",
    ")\n",
    "h_mpt_ag.shape, hlast_mpt_ag.shape  # , gates_mpt_ag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_mpt_ag, cst_mpt_ag = h_mpt_ag.unbind(dim=1)\n",
    "hst_mpt_ag.shape, cst_mpt_ag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_mpt_ag.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_mpt_ag.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch obw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx_mpt_obw = Wx.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "R_mpt_obw = R.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "b_mpt_obw = b.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "states_initial_mpt_obw = (\n",
    "    states_initial.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch实现的lstm？\n",
    "h_mpt_obw, hlast_mpt_obw = lstm_pt_fwbw(\n",
    "    states_initial=states_initial_mpt_obw,\n",
    "    Wx=Wx_mpt_obw,\n",
    "    R=R_mpt_obw,\n",
    "    b=b_mpt_obw,\n",
    "    autocast_kernel_dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_mpt_obw, cst_mpt_obw = h_mpt_obw.unbind(dim=1)\n",
    "hst_mpt_obw.shape, cst_mpt_obw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_mpt_obw.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hst_mpt_ag - hst_mpt_obw).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Wx_mpt_ag.grad - Wx_mpt_obw.grad).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(R_mpt_ag.grad - R_mpt_obw.grad).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(b_mpt_ag.grad - b_mpt_obw.grad).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(states_initial_mpt_ag.grad - states_initial_mpt_obw.grad).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx_mpt_ag, R_mpt_ag, b_mpt_ag, states_initial_mpt_ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_mpt_ag.grad, R_mpt_obw.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx_mpt_ag.grad, Wx_mpt_obw.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_mpt_ag.grad, b_mpt_obw.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    states_initial_mpt_ag.grad,\n",
    "    states_initial_mpt_obw.grad,\n",
    "    states_initial_mpt_ag.grad.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### triton impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx_mtr = Wx.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "R_mtr = R.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "b_mtr = b.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "states_initial_mtr = states_initial.clone().to(TGT_DTYPE).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triton实现的LSTM\n",
    "h_mtr, hlast_mtr = lstm_tr_fwbw(\n",
    "    states_initial=states_initial_mtr,\n",
    "    Wx=Wx_mtr,\n",
    "    R=R_mtr,\n",
    "    b=b_mtr,\n",
    "    autocast_kernel_dtype=\"float32\",\n",
    ")\n",
    "hlast_mtr.shape, h_mtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h_mtr - h_mpt_ag).abs().max(), h_mtr.shape, h_mpt_ag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlast_mtr.shape, hlast_mpt_ag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hlast_mtr - hlast_mpt_ag).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_mtr, cst_mtr = h_mtr.unbind(dim=1)\n",
    "hst_mtr.shape, cst_mtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_mtr.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Wx_mtr.grad - Wx_mpt_ag.grad).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (R_mtr.grad - R_mpt_ag.grad).abs().max(),\n",
    "    (R_mtr.grad - R_mpt_obw.grad).abs().max(),\n",
    "    (R_mpt_ag.grad - R_mpt_obw.grad).abs().max(),\n",
    "    R_mtr.grad.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(R_mtr.grad - R_mpt_ag.grad)[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (b_mtr.grad - b_mpt_ag.grad).abs().max(),\n",
    "    (b_mtr.grad - b_mpt_obw.grad).abs().max(),\n",
    "    (b_mpt_obw.grad - b_mpt_ag.grad).abs().max(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(states_initial_mtr.grad - states_initial_mpt_ag.grad).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_initial_mtr.grad[0], states_initial_mpt_ag.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_initial_mtr.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [flashrnn integration] Integrate LSTM torch_fwbw + triton fused into flashrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx_frnn = Wx.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "R_frnn = R.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "b_frnn = b.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "states_initial_frnn = states_initial.clone().to(TGT_DTYPE).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_frnn, hlast_frnn = flashrnn(\n",
    "    Wx=Wx_frnn,\n",
    "    R=R_frnn,\n",
    "    b=b_frnn,\n",
    "    states=None,  # states_initial_frnn,\n",
    "    function=\"lstm\",\n",
    "    backend=\"vanilla_fwbw\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "h_frnn.shape, h_mpt_ag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_frnn[0].sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_frnn_sh = rearrange(h_frnn, \"ns b t nh d -> t ns b nh d\")\n",
    "h_frnn_sh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h_frnn_sh - h_mpt_ag).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_frnn_sh.shape, h_mpt_ag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx_frnn_tr = Wx.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "R_frnn_tr = R.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "b_frnn_tr = b.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "states_initial_frnn_tr = (\n",
    "    states_initial.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_frnn_tr, hlast_frnn_tr = flashrnn(\n",
    "    Wx=Wx_frnn_tr,\n",
    "    R=R_frnn_tr,\n",
    "    b=b_frnn_tr,\n",
    "    states=None,  # states_initial_frnn,\n",
    "    function=\"lstm\",\n",
    "    backend=\"triton_fused\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "h_frnn_sh.shape, h_mpt_ag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_frnn_tr[0].sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_frnn_tr_sh = rearrange(h_frnn_tr, \"ns b t nh d -> t ns b nh d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h_frnn_tr_sh - h_mpt_ag).abs().max(), (h_frnn_tr_sh - h_mpt_ag).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_frnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick speed check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "TGT_DTYPE = torch.bfloat16\n",
    "B = 16  # batch size\n",
    "T = 1024  # sequence length\n",
    "NG = 4  # number of gates (NGI == NGR)\n",
    "NH = 1  # 1 #4      # number of heads\n",
    "D = 64  # input/hidden (embedding) dimension\n",
    "NS = 2  # number of states (c, h)\n",
    "\n",
    "###\n",
    "WARMUP_ITERS = 50\n",
    "ITERS = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx = torch.randn([B, T, NG, NH, D], device=device, dtype=dtype)\n",
    "R = torch.randn([NG, NH, D, D], device=device, dtype=dtype) / (D**0.5)\n",
    "b = torch.randn([NG, NH, D], device=device, dtype=dtype)\n",
    "states_initial = torch.randn([NS, B, NH, D], device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx_mpt_ag = Wx.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "R_mpt_ag = R.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "b_mpt_ag = b.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "states_initial_mpt_ag = (\n",
    "    states_initial.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    ")\n",
    "\n",
    "Wx_mtr = Wx.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "R_mtr = R.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "b_mtr = b.clone().to(TGT_DTYPE).detach().requires_grad_(True)\n",
    "states_initial_mtr = states_initial.clone().to(TGT_DTYPE).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch autograd baseline\n",
    "def lstm_pt_autograd():\n",
    "    h_mpt_ag, hlast_mpt_ag = forward_sequence(\n",
    "        states_initial=states_initial_mpt_ag,\n",
    "        Wx=Wx_mpt_ag,\n",
    "        R=R_mpt_ag,\n",
    "        b=b_mpt_ag,\n",
    "        forward_pointwise=lstm_pointwise_fw,\n",
    "        output_gates_and_states_initial=False,\n",
    "    )\n",
    "    hst_mpt_ag, cst_mpt_ag = h_mpt_ag.unbind(dim=1)\n",
    "    hst_mpt_ag.sum().backward()\n",
    "\n",
    "\n",
    "# triton fused kernel\n",
    "# lstm_tr_fwbw 和 flashrnn triton_fused 有什么区别\n",
    "def lstm_triton():\n",
    "    h_mtr, hlast_mtr = lstm_tr_fwbw(\n",
    "        states_initial=states_initial_mtr,\n",
    "        Wx=Wx_mtr,\n",
    "        R=R_mtr,\n",
    "        b=b_mtr,\n",
    "        autocast_kernel_dtype=\"float32\",\n",
    "    )\n",
    "    hst_mtr, cst_mtr = h_mtr.unbind(dim=1)\n",
    "    # hst_mtr.sum().backward()\n",
    "\n",
    "\n",
    "def lstm_triton_frnn():\n",
    "    h_mtr, hlast_mtr = flashrnn(\n",
    "        Wx=Wx_mtr,\n",
    "        R=R_mtr,\n",
    "        b=b_mtr,\n",
    "        states=None,  # states_initial_mtr,\n",
    "        function=\"lstm\",\n",
    "        backend=\"triton_fused\",\n",
    "        dtype=\"bfloat16\",\n",
    "    )\n",
    "    # h_mtr[0].sum().backward()\n",
    "\n",
    "\n",
    "# cuda fused kernel\n",
    "def lstm_cuda_fused():\n",
    "    out = flashrnn(\n",
    "        Wx=Wx_mtr,\n",
    "        R=R_mtr,\n",
    "        b=b_mtr,\n",
    "        function=\"lstm\",\n",
    "        dtype=\"bfloat16\",\n",
    "        backend=\"cuda_fused\",\n",
    "    )\n",
    "    out[0][0].sum().backward()\n",
    "\n",
    "\n",
    "torch_lstm = torch.nn.LSTM(\n",
    "    D, D, 1, bias=True, batch_first=False, bidirectional=False\n",
    ").to(device=device, dtype=dtype)\n",
    "pt_in = (\n",
    "    torch.randn([T, B, D], device=device, dtype=dtype)\n",
    "    .clone()\n",
    "    .detach()\n",
    "    .requires_grad_(True)\n",
    ")\n",
    "print(torch_lstm)\n",
    "print(pt_in.shape)\n",
    "\n",
    "\n",
    "def lstm_pt_fused_cuda():\n",
    "    out = torch_lstm(pt_in)\n",
    "    out[0].sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(WARMUP_ITERS), desc=\"Warmup - Triton\"):\n",
    "    lstm_triton()\n",
    "\n",
    "for _ in tqdm(range(ITERS), desc=\"Main - Triton\"):\n",
    "    lstm_triton()\n",
    "\n",
    "for _ in tqdm(range(WARMUP_ITERS), desc=\"Warmup - Triton\"):\n",
    "    lstm_triton_frnn()\n",
    "\n",
    "for _ in tqdm(range(ITERS), desc=\"Main - Triton\"):\n",
    "    lstm_triton_frnn()\n",
    "\n",
    "# for _ in tqdm(range(WARMUP_ITERS), desc=\"Warmup - Torch\"):\n",
    "#     lstm_pt_autograd()\n",
    "\n",
    "# for _ in tqdm(range(WARMUP_ITERS), desc=\"Main - Torch\"):\n",
    "#     lstm_pt_autograd()\n",
    "\n",
    "for _ in tqdm(range(WARMUP_ITERS), desc=\"Warmup - CUDA fused\"):\n",
    "    lstm_cuda_fused()\n",
    "\n",
    "for _ in tqdm(range(ITERS), desc=\"Warmup - CUDA fused\"):\n",
    "    lstm_cuda_fused()\n",
    "\n",
    "for _ in tqdm(range(WARMUP_ITERS), desc=\"Warmup - Torch CUDA fused\"):\n",
    "    lstm_pt_fused_cuda()\n",
    "\n",
    "for _ in tqdm(range(ITERS), desc=\"Warmup - Torch CUDA fused\"):\n",
    "    lstm_pt_fused_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main - Triton:   2%|▏         | 22/1000 [00:00<00:04, 213.30it/s]\n",
    "# Main - Triton: 100%|██████████| 1000/1000 [00:14<00:00, 68.19it/s]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt240cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
