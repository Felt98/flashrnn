{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ build cuda fused ==============\n",
      "file path: /mnt/second/qinhaoping/repo/flashRNN/main/flashrnn/flashrnn\n",
      "file path: /mnt/second/qinhaoping/repo/flashRNN/main/flashrnn/flashrnn\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.append(os.path.abspath(os.path.join(__file__, \"../../..\")))\n",
    "\n",
    "\n",
    "from flashrnn.frameworks.cuda_alternating.lstm import LSTMCuda\n",
    "from flashrnn.frameworks.cuda_fused.lstm import LSTMFused\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "dtype_str = \"bfloat16\"\n",
    "###\n",
    "# Config\n",
    "input_size = 1\n",
    "hidden_size = 32\n",
    "batch = 8\n",
    "num_layers = 1\n",
    "num_head = 1\n",
    "num_gate = 4\n",
    "requires_grad = True\n",
    "total_elems = 4*hidden_size*hidden_size\n",
    "\n",
    "R_g = torch.randn(total_elems, device=device,\n",
    "            dtype=dtype,\n",
    "            requires_grad=requires_grad,)  # 随机初始化一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_ref_lstm_constant(ref_lstm, value=1.0):\n",
    "    with torch.no_grad():\n",
    "        for name, param in ref_lstm.named_parameters():\n",
    "            param.fill_(value)\n",
    "\n",
    "\n",
    "def sync_from_pytorch_lstm(my_lstm: LSTMFused, ref_lstm: nn.LSTM, fused: bool):\n",
    "\n",
    "    \"\"\"\n",
    "    同步 nn.LSTM 的第一层权重到自定义的 LSTMFused。\n",
    "    要求：\n",
    "    - my_lstm.num_heads == 1\n",
    "    - my_lstm.num_layers == 1\n",
    "    - ref_lstm.num_layers == 1，单向\n",
    "    \"\"\"\n",
    "    assert my_lstm.num_heads == 1, \"只能同步 num_heads == 1 的模型\"\n",
    "    assert my_lstm.num_layers == 1, \"只能同步单层模型\"\n",
    "    assert (\n",
    "        ref_lstm.num_layers == 1 and not ref_lstm.bidirectional\n",
    "    ), \"只支持同步单层单向 LSTM\"\n",
    "\n",
    "    H = my_lstm.hidden_size\n",
    "    I = my_lstm.linear.in_features  # 输入维度\n",
    "    # 初始化 ref_lstm 的 recurrent 权重（保持 nn.Parameter 类型）\n",
    "    with torch.no_grad():\n",
    "\n",
    "\n",
    "        # R = R=torch.cat([torch.ones((total_elems//2,),device=device,dtype=dtype),torch.full((total_elems//2,),2.0,device=device,dtype=dtype)],dim=0)\n",
    "\n",
    "        ref_lstm.weight_hh_l0.data = R_g.view(4*H, H)\n",
    "        # ref_lstm.weight_hh_l0.data = torch.randn(\n",
    "        #     4 * H, H, device=device, dtype=dtype, requires_grad=requires_grad\n",
    "        # )\n",
    "        print(\"R:\",R_g)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ref_lstm.bias_ih_l0.zero_()\n",
    "        # ref_lstm.bias_hh_l0.zero_()\n",
    "        # ========== 1. 同步 Linear 权重 ==========\n",
    "        # ref: weight_ih_l0: [4H, I]\n",
    "        my_lstm.linear.weight.copy_(ref_lstm.weight_ih_l0)  # [4H, I]\n",
    "        my_lstm.linear.bias.copy_(ref_lstm.bias_ih_l0)  # [4H]\n",
    "\n",
    "        # ========== 2. 同步 Recurrent 权重 R ==========\n",
    "        weight_hh = ref_lstm.weight_hh_l0  # shape [4H, H]\n",
    "        gates = torch.split(weight_hh, H, dim=0)  # 4 tensors of shape [H, H]\n",
    "        stacked = torch.stack(gates, dim=0)  # [4, H, H]\n",
    "        R = stacked.unsqueeze(0).permute(0, 2, 1, 3).contiguous()  # [1, H, 4, H]\n",
    "        my_lstm.recurrents[0].copy_(R)\n",
    "        print(R)\n",
    "\n",
    "        # ========== 3. 同步 bias ==========\n",
    "        if fused:\n",
    "            total_bias = ref_lstm.bias_hh_l0  # shape [4H]\n",
    "            gates_b = torch.split(total_bias, H, dim=0)  # 4 tensors of shape [H]\n",
    "            b_stacked = (\n",
    "                torch.stack(gates_b, dim=0).unsqueeze(0).permute(0, 2, 1)\n",
    "            )  # [1, H, 4]\n",
    "            my_lstm.biases[0].copy_(b_stacked)\n",
    "        else:\n",
    "            total_bias =  ref_lstm.bias_hh_l0  # shape [4H]\n",
    "            gates_b = torch.split(total_bias, H, dim=0)  # 4 tensors of shape [H]\n",
    "            b_stacked = (\n",
    "                torch.stack(gates_b, dim=0).unsqueeze(0).permute(0, 1, 2)\n",
    "            )  # [1, H, 4]\n",
    "            my_lstm.biases[0].copy_(b_stacked)\n",
    "\n",
    "        # ========== 验证是否同步成功 ==========\n",
    "        # [4H, I]\n",
    "        diff_w = (my_lstm.linear.weight - ref_lstm.weight_ih_l0).abs().max()\n",
    "        print(f\"[Check] Linear weight max abs diff: {diff_w:.2e}\")\n",
    "\n",
    "        # [4H]\n",
    "        expected_bias = ref_lstm.bias_ih_l0\n",
    "        diff_b = (my_lstm.linear.bias - expected_bias).abs().max()\n",
    "        print(f\"[Check] Linear bias   max abs diff: {diff_b:.2e}\")\n",
    "\n",
    "        # [1, H, 4, H] -> [4, H, H]\n",
    "        R = my_lstm.recurrents[0].permute(2, 1, 3, 0).squeeze(3)  # [4, H, H]\n",
    "        R_flat = torch.cat([R[i] for i in range(4)], dim=0)  # [4H, H]\n",
    "        diff_R = (R_flat - ref_lstm.weight_hh_l0).abs().max()\n",
    "        print(f\"[Check] Recurrent weight max abs diff: {diff_R:.2e}\")\n",
    "\n",
    "        # [1, H, 4] -> [4H]\n",
    "        b_my = my_lstm.biases[0].permute(2, 1, 0).reshape(-1)  # [4H]\n",
    "        b_ref = ref_lstm.bias_hh_l0\n",
    "        diff_bias = (b_my - b_ref).abs().max()\n",
    "        print(f\"[Check] Bias max abs diff: {diff_bias:.2e}\")\n",
    "    print(\"[✓] LSTMFused 参数成功同步自 nn.LSTM。\")\n",
    "\n",
    "\n",
    "def max_abs_diff(a, b):\n",
    "    return (a - b).abs().max().item()\n",
    "\n",
    "\n",
    "def mean_abs_diff(a, b):\n",
    "    return (a - b).abs().mean().item()\n",
    "\n",
    "\n",
    "def max_ref_diff(a, b, eps=1e-8):\n",
    "    return ((a - b).abs() / (b.abs() + eps)).max().item()\n",
    "\n",
    "\n",
    "def mean_ref_diff(a, b, eps=1e-8):\n",
    "    return ((a - b).abs() / (b.abs() + eps)).mean().item()\n",
    "\n",
    "def print_lstm_all_params(lstm: nn.LSTM):\n",
    "    with torch.no_grad():\n",
    "        for name, param in lstm.named_parameters():\n",
    "            print(\"name :\",name)\n",
    "            print(\"param:\",param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: tensor([-0.9258, -0.4258, -2.6406,  ...,  1.6016, -1.0703,  1.6016],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)\n",
      "tensor([[[[-0.9258, -0.4258, -2.6406,  ..., -0.8359,  1.3516, -0.2871],\n",
      "          [-1.0312, -0.8906, -0.1914,  ..., -1.3125,  0.5312, -0.7266],\n",
      "          [ 1.4922, -1.5703,  0.3320,  ...,  0.0315,  0.1030, -1.0156],\n",
      "          [-0.1016, -0.3477,  0.6562,  ..., -2.2344, -0.3496,  0.9023]],\n",
      "\n",
      "         [[-0.5977, -0.3281, -0.9102,  ...,  0.1211,  0.4727, -1.0859],\n",
      "          [ 0.0554, -0.6758,  1.2969,  ...,  0.8711, -0.4844,  0.1631],\n",
      "          [-1.1562,  1.7109,  0.4316,  ...,  1.0703,  1.0625,  0.9023],\n",
      "          [ 0.7773,  0.2119, -0.6133,  ...,  1.3281, -0.8867,  1.1641]],\n",
      "\n",
      "         [[-0.0334, -0.9727,  0.9570,  ...,  0.5234,  1.1719, -0.9570],\n",
      "          [ 0.1157, -0.8398, -1.0078,  ...,  0.0693, -0.3086, -0.0967],\n",
      "          [ 0.7930,  0.2129,  0.8047,  ...,  0.0074, -0.9102, -0.0879],\n",
      "          [ 0.1953, -0.8555, -1.0312,  ..., -0.1104, -0.6406, -0.2500]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5859,  0.7383, -0.2578,  ..., -2.1719, -1.0391, -0.9648],\n",
      "          [ 0.5156, -0.2520,  0.5117,  ..., -0.6406,  0.5039,  0.0457],\n",
      "          [ 1.8750,  0.2891, -0.7227,  ..., -2.8281,  0.2578,  0.1631],\n",
      "          [ 1.2500,  0.5430,  1.7969,  ..., -0.5312,  0.0552, -0.0664]],\n",
      "\n",
      "         [[-0.7773,  0.1846, -1.5703,  ...,  0.5078, -1.5469,  0.1797],\n",
      "          [-0.7422, -1.9375,  0.0160,  ...,  0.5977, -0.5586, -1.5859],\n",
      "          [-0.1572,  0.3633,  1.6406,  ..., -0.7969, -0.2236, -0.4746],\n",
      "          [ 0.6289,  0.9922,  0.9062,  ...,  0.5664, -0.8477, -0.7266]],\n",
      "\n",
      "         [[ 0.5703,  0.5547,  0.3066,  ..., -1.2344, -1.2031, -0.1084],\n",
      "          [ 0.1196,  1.2500,  0.9844,  ...,  1.4688, -0.0562,  0.0457],\n",
      "          [ 1.4922,  0.8203,  0.7773,  ..., -0.6602,  2.3125,  0.4824],\n",
      "          [-0.8711, -0.2695, -0.8086,  ...,  1.6016, -1.0703,  1.6016]]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "[Check] Linear weight max abs diff: 0.00e+00\n",
      "[Check] Linear bias   max abs diff: 0.00e+00\n",
      "[Check] Recurrent weight max abs diff: 0.00e+00\n",
      "[Check] Bias max abs diff: 0.00e+00\n",
      "[✓] LSTMFused 参数成功同步自 nn.LSTM。\n",
      "state is leaf? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/qinhaoping/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/qinhaoping/.cache/torch_extensions/py311_cu118/lstm_HS32BS8NH1NS2DbDBbDRbDWbDGbDSbDAbNGR4NGW4NGI4NGT4SA1UDB1GRCV0GR/build.ninja...\n",
      "/mnt/second/qinhaoping/anaconda3/envs/flashrnn/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module lstm_HS32BS8NH1NS2DbDBbDRbDWbDGbDSbDAbNGR4NGW4NGI4NGT4SA1UDB1GRCV0GR...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "out_my shape:  torch.Size([8, 2, 32])\n",
      "out_ref shape:  torch.Size([8, 2, 32])\n",
      "grads: [torch.Size([2, 8, 1, 4, 32]), torch.Size([2, 8, 1, 32]), torch.Size([1, 32, 4, 32]), torch.Size([1, 4, 32])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module lstm_HS32BS8NH1NS2DbDBbDRbDWbDGbDSbDAbNGR4NGW4NGI4NGT4SA1UDB1GRCV0GR...\n"
     ]
    }
   ],
   "source": [
    "seq_len = 2\n",
    "\n",
    "\n",
    "# Models\n",
    "ref_lstm = nn.LSTM(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    bias=True,\n",
    "    batch_first=True,\n",
    "    bidirectional=False,\n",
    ").to(device=device, dtype=dtype)\n",
    "my_lstm = LSTMCuda(input_size, hidden_size, num_layers).to(\n",
    "    device=device, dtype=dtype\n",
    ")\n",
    "fused = False\n",
    "initialize_ref_lstm_constant(ref_lstm)\n",
    "# print_lstm_all_params(ref_lstm)\n",
    "sync_from_pytorch_lstm(my_lstm, ref_lstm, fused)  # 同步权重\n",
    "\n",
    "# Inputs\n",
    "# x = torch.randn(batch, seq_len, input_size, device=\"cuda\", requires_grad=False)\n",
    "x = torch.randn(\n",
    "    batch, seq_len, input_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "h0 = torch.zeros(\n",
    "    num_layers, batch, hidden_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "c0 = torch.zeros(\n",
    "    num_layers, batch, hidden_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "\n",
    "# Clone inputs for reference\n",
    "x_ref = x.detach().clone().requires_grad_()\n",
    "h0_ref = h0.detach().clone().requires_grad_()\n",
    "c0_ref = c0.detach().clone().requires_grad_()\n",
    "\n",
    "# Forward\n",
    "# out_ref: [B,T,H]\n",
    "# hn_ref \\ cn_ref: [num_layers,B,H]\n",
    "out_ref, (hn_ref, cn_ref) = ref_lstm(x_ref, (h0_ref, c0_ref))\n",
    "out_my, (hn_my, cn_my) = my_lstm(x, (h0, c0))\n",
    "# out_ref, (hn_ref, cn_ref) = ref_lstm(x_ref)\n",
    "# out_my, (hn_my, cn_my) = my_lstm(x)\n",
    "# print(\"out my: \", out_my)\n",
    "# print(\"out ref:\", out_ref)\n",
    "print(\"out_my shape: \", out_my.shape)\n",
    "print(\"out_ref shape: \", out_ref.shape)\n",
    "# Backward\n",
    "loss_my = out_my.sum()\n",
    "loss_ref = out_ref.sum()\n",
    "loss_my.backward()\n",
    "loss_ref.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2812,  1.3750, -0.3105,  0.2539,  0.6758,  0.5000,  0.8711,\n",
      "          -0.0640,  1.6172,  0.1157,  1.1797,  0.0364, -0.5430,  1.4844,\n",
      "          -0.4043,  0.7383,  1.3672,  1.1484,  0.8438,  0.0378,  0.1289,\n",
      "           1.5625,  0.7305,  1.2188, -0.4492,  0.9258, -0.2324,  1.0312,\n",
      "          -0.7500,  0.5625,  1.5859,  1.7578],\n",
      "         [ 0.2578,  1.3281, -0.4453,  0.3945,  0.5312,  0.5938,  0.9258,\n",
      "          -0.0072,  1.6406,  0.0222,  1.1328,  0.0179, -0.4844,  1.4844,\n",
      "          -0.4707,  0.8477,  1.3203,  1.1016,  0.9219, -0.2422,  0.2236,\n",
      "           1.5781,  0.6250,  1.1406, -0.4766,  0.8203, -0.1992,  1.0156,\n",
      "          -0.8047,  0.4609,  1.5938,  1.8203],\n",
      "         [ 1.1406,  1.7812,  0.6211,  0.1167,  1.2422,  0.5469,  1.0078,\n",
      "          -0.0192,  1.8828,  0.5664,  1.4141,  0.0713, -0.6719,  1.8125,\n",
      "           0.1934,  0.8828,  1.6797,  1.3516,  0.9414,  0.8281,  0.2100,\n",
      "           1.8672,  1.2969,  1.6719, -0.1875,  1.4844, -0.0664,  1.4609,\n",
      "          -0.4746,  0.8945,  1.8828,  1.9375],\n",
      "         [ 0.1953,  1.2266, -0.4043,  0.3359,  0.5234,  0.4863,  0.8398,\n",
      "          -0.0554,  1.5234,  0.0142,  1.1250,  0.0262, -0.4629,  1.3828,\n",
      "          -0.5195,  0.7461,  1.2891,  1.1016,  0.8320, -0.2578,  0.1836,\n",
      "           1.4531,  0.5625,  1.0469, -0.5156,  0.7539, -0.2754,  0.9102,\n",
      "          -0.8125,  0.4492,  1.4688,  1.7031],\n",
      "         [ 1.2188,  1.7656,  0.7383,  0.0854,  1.2812,  0.5234,  1.0000,\n",
      "          -0.0452,  1.8594,  0.6211,  1.4375,  0.0918, -0.6172,  1.7969,\n",
      "           0.2949,  0.8906,  1.6797,  1.3750,  0.9062,  0.8477,  0.2441,\n",
      "           1.8438,  1.3203,  1.6641, -0.2012,  1.4922, -0.0879,  1.4688,\n",
      "          -0.4531,  0.9258,  1.8516,  1.9141],\n",
      "         [ 0.7070,  1.2578,  0.5391, -0.0942,  0.9570,  0.2559,  0.7305,\n",
      "          -0.3242,  1.3359,  0.5742,  1.2500,  0.3008, -0.1963,  1.2812,\n",
      "           0.2441,  0.5430,  1.3516,  1.2422,  0.4121,  0.6016,  0.2471,\n",
      "           1.3047,  0.9023,  1.1875, -0.5078,  1.0234, -0.3340,  1.0078,\n",
      "          -0.5156,  0.8477,  1.3359,  1.4219],\n",
      "         [ 0.8438,  1.6641,  0.3613,  0.1162,  1.0859,  0.5039,  0.9531,\n",
      "          -0.0586,  1.7969,  0.4609,  1.3438,  0.0703, -0.6484,  1.7109,\n",
      "           0.0092,  0.7930,  1.5781,  1.2969,  0.8867,  0.6953,  0.1455,\n",
      "           1.7656,  1.1406,  1.5469, -0.2754,  1.3281, -0.1318,  1.3359,\n",
      "          -0.5586,  0.8203,  1.7891,  1.8672],\n",
      "         [ 0.7344,  1.3984,  0.4609, -0.0476,  0.9961,  0.2871,  0.7891,\n",
      "          -0.2412,  1.5000,  0.5273,  1.2969,  0.1855, -0.4180,  1.4297,\n",
      "           0.0938,  0.6016,  1.4297,  1.2734,  0.5898,  0.6133,  0.1514,\n",
      "           1.4688,  0.9727,  1.3203, -0.4512,  1.1172, -0.3184,  1.1172,\n",
      "          -0.5820,  0.8359,  1.4922,  1.5859]]], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "print(cn_ref)\n",
    "print(cn_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2812,  1.3750, -0.3105,  0.2539,  0.6758,  0.5000,  0.8711,\n",
      "          -0.0640,  1.6172,  0.1157,  1.1797,  0.0364, -0.5430,  1.4844,\n",
      "          -0.4043,  0.7383,  1.3672,  1.1484,  0.8438,  0.0378,  0.1289,\n",
      "           1.5625,  0.7305,  1.2188, -0.4492,  0.9258, -0.2324,  1.0312,\n",
      "          -0.7500,  0.5625,  1.5859,  1.7578],\n",
      "         [ 0.2578,  1.3281, -0.4453,  0.3945,  0.5312,  0.5938,  0.9258,\n",
      "          -0.0072,  1.6406,  0.0222,  1.1328,  0.0179, -0.4844,  1.4844,\n",
      "          -0.4707,  0.8477,  1.3203,  1.1016,  0.9219, -0.2422,  0.2236,\n",
      "           1.5781,  0.6250,  1.1406, -0.4766,  0.8203, -0.1992,  1.0156,\n",
      "          -0.8047,  0.4609,  1.5938,  1.8203],\n",
      "         [ 1.1406,  1.7812,  0.6211,  0.1167,  1.2422,  0.5469,  1.0078,\n",
      "          -0.0192,  1.8828,  0.5664,  1.4141,  0.0713, -0.6719,  1.8125,\n",
      "           0.1934,  0.8828,  1.6797,  1.3516,  0.9414,  0.8281,  0.2100,\n",
      "           1.8672,  1.2969,  1.6719, -0.1875,  1.4844, -0.0664,  1.4609,\n",
      "          -0.4746,  0.8945,  1.8828,  1.9375],\n",
      "         [ 0.1982,  1.2344, -0.4023,  0.3398,  0.5273,  0.4883,  0.8438,\n",
      "          -0.0530,  1.5234,  0.0143,  1.1250,  0.0264, -0.4629,  1.3828,\n",
      "          -0.5195,  0.7500,  1.2891,  1.1016,  0.8359, -0.2578,  0.1875,\n",
      "           1.4531,  0.5664,  1.0469, -0.5156,  0.7578, -0.2715,  0.9141,\n",
      "          -0.8125,  0.4492,  1.4688,  1.7031],\n",
      "         [ 1.2188,  1.7656,  0.7383,  0.0854,  1.2812,  0.5234,  1.0000,\n",
      "          -0.0452,  1.8594,  0.6211,  1.4375,  0.0918, -0.6172,  1.7969,\n",
      "           0.2949,  0.8906,  1.6797,  1.3750,  0.9062,  0.8477,  0.2441,\n",
      "           1.8438,  1.3203,  1.6641, -0.2012,  1.4922, -0.0879,  1.4688,\n",
      "          -0.4531,  0.9258,  1.8516,  1.9141],\n",
      "         [ 0.7070,  1.2578,  0.5391, -0.0942,  0.9570,  0.2559,  0.7305,\n",
      "          -0.3242,  1.3359,  0.5742,  1.2500,  0.3008, -0.1963,  1.2812,\n",
      "           0.2441,  0.5430,  1.3516,  1.2422,  0.4121,  0.6016,  0.2471,\n",
      "           1.3047,  0.9023,  1.1875, -0.5078,  1.0234, -0.3340,  1.0078,\n",
      "          -0.5156,  0.8477,  1.3359,  1.4219],\n",
      "         [ 0.8477,  1.6641,  0.3652,  0.1157,  1.0938,  0.5039,  0.9570,\n",
      "          -0.0586,  1.7969,  0.4629,  1.3438,  0.0708, -0.6484,  1.7109,\n",
      "           0.0117,  0.7930,  1.5859,  1.2969,  0.8867,  0.6992,  0.1455,\n",
      "           1.7734,  1.1406,  1.5469, -0.2734,  1.3281, -0.1318,  1.3359,\n",
      "          -0.5547,  0.8203,  1.7891,  1.8672],\n",
      "         [ 0.7344,  1.3984,  0.4629, -0.0479,  0.9961,  0.2871,  0.7891,\n",
      "          -0.2422,  1.5000,  0.5273,  1.2969,  0.1855, -0.4180,  1.4297,\n",
      "           0.0957,  0.6016,  1.4297,  1.2734,  0.5898,  0.6172,  0.1523,\n",
      "           1.4688,  0.9766,  1.3203, -0.4512,  1.1172, -0.3184,  1.1172,\n",
      "          -0.5820,  0.8398,  1.4922,  1.5859]]], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "print(cn_my)\n",
    "print(cn_my.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28125, 0.2578125, 1.140625, 0.1982421875, 1.21875, 0.70703125, 0.84765625, 0.734375]\n",
      "[0.28125, 0.2578125, 1.140625, 0.1953125, 1.21875, 0.70703125, 0.84375, 0.734375]\n"
     ]
    }
   ],
   "source": [
    "result1 = cn_my[0, :, 0].tolist()  # list of 16 elements\n",
    "result2 = cn_ref[0, :, 0].tolist()  # list of 16 elements\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Forward] Output max abs diff: 3.91e-03\n",
      "[Forward] hn     max abs diff: 3.91e-03\n",
      "[Forward] cn     max abs diff: 7.81e-03\n",
      "[Forward] Output mean abs diff: 1.05e-04\n",
      "[Forward] hn     mean abs diff: 2.10e-04\n",
      "[Forward] cn     mean abs diff: 4.86e-04\n",
      "[Forward] Output max ref diff: 2.81e-01\n",
      "[Forward] hn     max ref diff: 2.81e-01\n",
      "[Forward] cn     max ref diff: 2.73e-01\n",
      "[Forward] Output mean ref diff: 1.14e-03\n",
      "[Forward] hn     mean ref diff: 2.29e-03\n",
      "[Forward] cn     mean ref diff: 2.24e-03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Output comparison\n",
    "print(f\"[Forward] Output max abs diff: {max_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max abs diff: {max_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     max abs diff: {max_abs_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean abs diff: {mean_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean abs diff: {mean_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     mean abs diff: {mean_abs_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output max ref diff: {max_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max ref diff: {max_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     max ref diff: {max_ref_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean ref diff: {mean_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean ref diff: {mean_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     mean ref diff: {mean_ref_diff(cn_my, cn_ref):.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grad] Input x     grad diff: 3.12e-02\n",
      "[Grad] Input x     grad diff: 2.44e-03\n",
      "[Grad] Input x     grad diff: 6.96e-03\n",
      "[Grad] Input x     grad diff: 6.79e-04\n",
      "[Grad] h0          grad diff: 1.56e-02\n",
      "[Grad] c0          grad diff: 1.56e-02\n",
      "[Grad] Param linear.weight        grad diff: 3.91e-03\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (32) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (n1, p1), (n2, p2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m     11\u001b[39m     my_lstm.named_parameters(), ref_lstm.named_parameters()\n\u001b[32m     12\u001b[39m ):\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p1.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m p2.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         diff = \u001b[43mmax_abs_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Grad] Param \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m20s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m grad diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mmax_abs_diff\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmax_abs_diff\u001b[39m(a, b):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m).abs().max().item()\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (128) must match the size of tensor b (32) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gradients\n",
    "print(f\"[Grad] Input x     grad diff: {max_abs_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {mean_abs_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {max_ref_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {mean_ref_diff(x.grad, x_ref.grad):.2e}\")\n",
    "\n",
    "print(f\"[Grad] h0          grad diff: {max_abs_diff(h0.grad, h0_ref.grad):.2e}\")\n",
    "print(f\"[Grad] c0          grad diff: {max_abs_diff(c0.grad, c0_ref.grad):.2e}\")\n",
    "\n",
    "for (n1, p1), (n2, p2) in zip(\n",
    "    my_lstm.named_parameters(), ref_lstm.named_parameters()\n",
    "):\n",
    "    if p1.grad is not None and p2.grad is not None:\n",
    "        diff = max_abs_diff(p1.grad, p2.grad)\n",
    "        print(f\"[Grad] Param {n1:20s} grad diff: {diff:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrnn.flashrnn import flashrnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 2\n",
    "DH = hidden_size // num_head\n",
    "NH=num_head\n",
    "\n",
    "gate_linear = torch.nn.Linear(input_size, num_gate * hidden_size).to(\n",
    "    device=device, dtype=dtype\n",
    ")\n",
    "with torch.no_grad():\n",
    "    gate_linear.weight.fill_(1.0)\n",
    "    if gate_linear.bias is not None:\n",
    "        gate_linear.bias.fill_(1.0)\n",
    "\n",
    "x = torch.randn(\n",
    "    batch, seq_len, input_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "total_elems = 4*NH*DH*DH\n",
    "# R=torch.cat([torch.ones((total_elems//2,),device=device,dtype=dtype),torch.full((total_elems//2,),2.0,device=device,dtype=dtype)],dim=0)\n",
    "# R.requires_grad_(requires_grad)\n",
    "print(\"R_g:\",R_g)\n",
    "R=R_g.view(num_gate, NH, DH, DH)\n",
    "print(R.shape)\n",
    "print(R)\n",
    "# R = torch.randn(\n",
    "#     [num_gate, NH, DH, DH],\n",
    "#     # [NH,DH,num_gate,DH],\n",
    "\n",
    "#     device=device,\n",
    "#     dtype=dtype,\n",
    "#     requires_grad=requires_grad,\n",
    "# ) \n",
    "\n",
    "b = torch.ones(\n",
    "    [num_gate, NH, DH],\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    requires_grad=requires_grad,\n",
    ")\n",
    "R_mtr = R.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "b_mtr = b.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "\n",
    "Wx = gate_linear(x)\n",
    "Wx = Wx.reshape(\n",
    "    Wx.shape[0], Wx.shape[1], R.shape[0], R.shape[1], R.shape[2]\n",
    ")\n",
    "# Wx = Wx.reshape(\n",
    "#     seq_len, batch, num_head, DH, num_gate\n",
    "# )\n",
    "Wx_mtr = Wx.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "\n",
    "\n",
    "print(Wx.shape)\n",
    "r=R\n",
    "r=r.permute(1,3,0,2)\n",
    "print(r)\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "ref_lstm = nn.LSTM(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    bias=True,\n",
    "    batch_first=True,\n",
    "    bidirectional=False,\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "initialize_ref_lstm_constant(ref_lstm)\n",
    "\n",
    "# ========== 2. 同步 Recurrent 权重 R ==========\n",
    "# 转换成 [4H, H] 形式，用于赋值给 ref_lstm.weight_hh_l0\n",
    "# 步骤：\n",
    "R_perm = R         # [4, NH, D, D]\n",
    "R_reshaped = R_perm.reshape(4, hidden_size, hidden_size)  # [4, H, D]\n",
    "weight_hh = R_reshaped.reshape(4 * hidden_size, hidden_size)  # [4H, D]\n",
    "# 赋值到 ref_lstm 的 recurrent 权重\n",
    "ref_lstm.weight_hh_l0.data.copy_(weight_hh)\n",
    "print(\"ref_lstm.weight_hh_l0.data:\",ref_lstm.weight_hh_l0.data)\n",
    "\n",
    "\n",
    "# Inputs\n",
    "# x = torch.randn(batch, seq_len, input_size, device=\"cuda\", requires_grad=False)\n",
    "h0 = torch.zeros(\n",
    "    num_layers, batch, hidden_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "c0 = torch.zeros(\n",
    "    num_layers, batch, hidden_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "\n",
    "# Clone inputs for reference\n",
    "x_ref = x.detach().clone().requires_grad_()\n",
    "h0_ref = h0.detach().clone().requires_grad_()\n",
    "c0_ref = c0.detach().clone().requires_grad_()\n",
    "\n",
    "# Forward\n",
    "out_ref, (hn_ref, cn_ref) = ref_lstm(x_ref, (h0_ref, c0_ref))      #\n",
    "# out_my, last_h = flashrnn(Wx,R,b, function=\"lstm\",backend=\"cuda_fused\",dtype=dtype_str)\n",
    "out_my, (hn_my, cn_my) = flashrnn(Wx,R,b, function=\"lstm\",backend=\"cuda\",dtype=dtype_str)\n",
    "out_my=out_my.reshape(batch, seq_len, hidden_size)\n",
    "hn_my=hn_my.reshape(num_layers, batch, hidden_size)\n",
    "cn_my=cn_my.reshape(num_layers, batch, hidden_size)\n",
    "\n",
    "# out_ref, (hn_ref, cn_ref) = ref_lstm(x_ref)\n",
    "# out_my, (hn_my, cn_my) = my_lstm(x)\n",
    "# print(\"out my: \", out_my)\n",
    "# print(\"out ref:\", out_ref)\n",
    "print(\"out_my shape: \", out_my.shape)    #  [NS, B,T,NH,D]\n",
    "print(\"out_ref shape: \", out_ref.shape)  # [B,T,H]\n",
    "# # Backward\n",
    "# loss_my = out_my.sum()\n",
    "# loss_ref = out_ref.sum()\n",
    "# loss_my.backward()\n",
    "# loss_ref.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_my=cn_my.reshape(num_layers, batch, hidden_size)\n",
    "print(cn_my.shape)\n",
    "\n",
    "print(cn_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = cn_my[0, :, 0].tolist()  # list of 16 elements\n",
    "result2 = cn_ref[0, :, 0].tolist()  # list of 16 elements\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output comparison\n",
    "print(f\"[Forward] Output max abs diff: {max_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max abs diff: {max_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     max abs diff: {max_abs_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean abs diff: {mean_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean abs diff: {mean_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     mean abs diff: {mean_abs_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output max ref diff: {max_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max ref diff: {max_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     max ref diff: {max_ref_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean ref diff: {mean_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean ref diff: {mean_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     mean ref diff: {mean_ref_diff(cn_my, cn_ref):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flashrnn)",
   "language": "python",
   "name": "flashrnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
