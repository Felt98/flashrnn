{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.append(os.path.abspath(os.path.join(__file__, \"../../..\")))\n",
    "\n",
    "\n",
    "from flashrnn.frameworks.cuda_alternating.gru import GRUCuda\n",
    "from flashrnn.frameworks.cuda_fused.gru import GRUFused\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "dtype_str = \"bfloat16\"\n",
    "###\n",
    "# Config\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "batch = 16\n",
    "num_layers = 1\n",
    "num_head = 1\n",
    "num_gate = 3\n",
    "requires_grad = True\n",
    "total_elems = 3*hidden_size*hidden_size\n",
    "\n",
    "R_g = torch.randn(total_elems, device=device,\n",
    "            dtype=dtype,\n",
    "            requires_grad=requires_grad,)  # 随机初始化一次\n",
    "# R_g = torch.cat([torch.ones((total_elems//2,),device=device,dtype=dtype),torch.full((total_elems//2,),2.0,device=device,dtype=dtype)],dim=0)\n",
    "# R_g = torch.ones(total_elems, device=device,\n",
    "#             dtype=dtype,\n",
    "#             requires_grad=requires_grad,)  # 随机初始化一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_ref_lstm_constant(ref_lstm, value=1.0):\n",
    "    with torch.no_grad():\n",
    "        for name, param in ref_lstm.named_parameters():\n",
    "            param.fill_(value)\n",
    "\n",
    "\n",
    "def sync_from_pytorch_lstm(my_lstm, ref_lstm: nn.GRU, fused: bool):\n",
    "\n",
    "    \"\"\"\n",
    "    同步 nn.LSTM 的第一层权重到自定义的 LSTMFused。\n",
    "    要求：\n",
    "    - my_lstm.num_heads == 1\n",
    "    - my_lstm.num_layers == 1\n",
    "    - ref_lstm.num_layers == 1，单向\n",
    "    \"\"\"\n",
    "    assert my_lstm.num_heads == 1, \"只能同步 num_heads == 1 的模型\"\n",
    "    assert my_lstm.num_layers == 1, \"只能同步单层模型\"\n",
    "    assert (\n",
    "        ref_lstm.num_layers == 1 and not ref_lstm.bidirectional\n",
    "    ), \"只支持同步单层单向 LSTM\"\n",
    "\n",
    "    H = my_lstm.hidden_size\n",
    "    I = my_lstm.linear.in_features  # 输入维度\n",
    "    # 初始化 ref_lstm 的 recurrent 权重（保持 nn.Parameter 类型）\n",
    "    with torch.no_grad():\n",
    "\n",
    "\n",
    "        # R = R=torch.cat([torch.ones((total_elems//2,),device=device,dtype=dtype),torch.full((total_elems//2,),2.0,device=device,dtype=dtype)],dim=0)\n",
    "\n",
    "        ref_lstm.weight_hh_l0.data = R_g.view(3*H, H)\n",
    "        # ref_lstm.weight_hh_l0.data = torch.randn(\n",
    "        #     4 * H, H, device=device, dtype=dtype, requires_grad=requires_grad\n",
    "        # )\n",
    "        print(\"R:\",R_g)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ref_lstm.bias_ih_l0.zero_()\n",
    "        # ref_lstm.bias_hh_l0.zero_()\n",
    "        # ========== 1. 同步 Linear 权重 ==========\n",
    "        # ref: weight_ih_l0: [4H, I]\n",
    "        my_lstm.linear.weight.copy_(ref_lstm.weight_ih_l0)  # [4H, I]\n",
    "        my_lstm.linear.bias.copy_(ref_lstm.bias_ih_l0)  # [4H]\n",
    "\n",
    "        # ========== 2. 同步 Recurrent 权重 R ==========\n",
    "        weight_hh = ref_lstm.weight_hh_l0  # shape [4H, H]\n",
    "        gates = torch.split(weight_hh, H, dim=0)  # 4 tensors of shape [H, H]\n",
    "        stacked = torch.stack(gates, dim=0)  # [4, H, H]\n",
    "        R = stacked.unsqueeze(0).permute(0, 2, 1, 3).contiguous()  # [1, H, 4, H]\n",
    "        my_lstm.recurrents[0].copy_(R)\n",
    "        print(R)\n",
    "\n",
    "        # ========== 3. 同步 bias ==========\n",
    "        if fused:\n",
    "            total_bias = ref_lstm.bias_hh_l0  # shape [4H]\n",
    "            gates_b = torch.split(total_bias, H, dim=0)  # 4 tensors of shape [H]\n",
    "            b_stacked = (\n",
    "                torch.stack(gates_b, dim=0).unsqueeze(0).permute(0, 2, 1)\n",
    "            )  # [1, H, 4]\n",
    "            my_lstm.biases[0].copy_(b_stacked)\n",
    "        else:\n",
    "            total_bias =  ref_lstm.bias_hh_l0  # shape [4H]\n",
    "            gates_b = torch.split(total_bias, H, dim=0)  # 4 tensors of shape [H]\n",
    "            b_stacked = (\n",
    "                torch.stack(gates_b, dim=0).unsqueeze(0).permute(0, 1, 2)\n",
    "            )  # [1, H, 4]\n",
    "            my_lstm.biases[0].copy_(b_stacked)\n",
    "\n",
    "        # ========== 验证是否同步成功 ==========\n",
    "        # [4H, I]\n",
    "        diff_w = (my_lstm.linear.weight - ref_lstm.weight_ih_l0).abs().max()\n",
    "        print(f\"[Check] Linear weight max abs diff: {diff_w:.2e}\")\n",
    "\n",
    "        # [4H]\n",
    "        expected_bias = ref_lstm.bias_ih_l0\n",
    "        diff_b = (my_lstm.linear.bias - expected_bias).abs().max()\n",
    "        print(f\"[Check] Linear bias   max abs diff: {diff_b:.2e}\")\n",
    "\n",
    "        # [1, H, 4, H] -> [4, H, H]\n",
    "        R = my_lstm.recurrents[0].permute(2, 1, 3, 0).squeeze(3)  # [4, H, H]\n",
    "        R_flat = torch.cat([R[i] for i in range(3)], dim=0)  # [4H, H]\n",
    "        diff_R = (R_flat - ref_lstm.weight_hh_l0).abs().max()\n",
    "        print(f\"[Check] Recurrent weight max abs diff: {diff_R:.2e}\")\n",
    "\n",
    "        # [1, H, 4] -> [4H]\n",
    "        b_my = my_lstm.biases[0].permute(2, 1, 0).reshape(-1)  # [4H]\n",
    "        b_ref = ref_lstm.bias_hh_l0\n",
    "        diff_bias = (b_my - b_ref).abs().max()\n",
    "        print(f\"[Check] Bias max abs diff: {diff_bias:.2e}\")\n",
    "    print(\"[✓] LSTMFused 参数成功同步自 nn.LSTM。\")\n",
    "\n",
    "\n",
    "def max_abs_diff(a, b):\n",
    "    return (a - b).abs().max().item()\n",
    "\n",
    "\n",
    "def mean_abs_diff(a, b):\n",
    "    return (a - b).abs().mean().item()\n",
    "\n",
    "\n",
    "def max_ref_diff(a, b, eps=1e-8):\n",
    "    return ((a - b).abs() / (b.abs() + eps)).max().item()\n",
    "\n",
    "\n",
    "def mean_ref_diff(a, b, eps=1e-8):\n",
    "    return ((a - b).abs() / (b.abs() + eps)).mean().item()\n",
    "\n",
    "def print_lstm_all_params(lstm: nn.LSTM):\n",
    "    with torch.no_grad():\n",
    "        for name, param in lstm.named_parameters():\n",
    "            print(\"name :\",name)\n",
    "            print(\"param:\",param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: tensor([-0.9258, -0.4258, -2.6406,  ..., -0.6602,  2.3125,  0.4824],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)\n",
      "tensor([[[[-0.9258, -0.4258, -2.6406,  ..., -0.8359,  1.3516, -0.2871],\n",
      "          [-1.0312, -0.8906, -0.1914,  ..., -1.3125,  0.5312, -0.7266],\n",
      "          [ 1.4922, -1.5703,  0.3320,  ...,  0.0315,  0.1030, -1.0156]],\n",
      "\n",
      "         [[-0.5977, -0.3281, -0.9102,  ...,  0.1211,  0.4727, -1.0859],\n",
      "          [ 0.0554, -0.6758,  1.2969,  ...,  0.8711, -0.4844,  0.1631],\n",
      "          [-1.1562,  1.7109,  0.4316,  ...,  1.0703,  1.0625,  0.9023]],\n",
      "\n",
      "         [[-0.0334, -0.9727,  0.9570,  ...,  0.5234,  1.1719, -0.9570],\n",
      "          [ 0.1157, -0.8398, -1.0078,  ...,  0.0693, -0.3086, -0.0967],\n",
      "          [ 0.7930,  0.2129,  0.8047,  ...,  0.0074, -0.9102, -0.0879]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5859,  0.7383, -0.2578,  ..., -2.1719, -1.0391, -0.9648],\n",
      "          [ 0.5156, -0.2520,  0.5117,  ..., -0.6406,  0.5039,  0.0457],\n",
      "          [ 1.8750,  0.2891, -0.7227,  ..., -2.8281,  0.2578,  0.1631]],\n",
      "\n",
      "         [[-0.7773,  0.1846, -1.5703,  ...,  0.5078, -1.5469,  0.1797],\n",
      "          [-0.7422, -1.9375,  0.0160,  ...,  0.5977, -0.5586, -1.5859],\n",
      "          [-0.1572,  0.3633,  1.6406,  ..., -0.7969, -0.2236, -0.4746]],\n",
      "\n",
      "         [[ 0.5703,  0.5547,  0.3066,  ..., -1.2344, -1.2031, -0.1084],\n",
      "          [ 0.1196,  1.2500,  0.9844,  ...,  1.4688, -0.0562,  0.0457],\n",
      "          [ 1.4922,  0.8203,  0.7773,  ..., -0.6602,  2.3125,  0.4824]]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "[Check] Linear weight max abs diff: 0.00e+00\n",
      "[Check] Linear bias   max abs diff: 0.00e+00\n",
      "[Check] Recurrent weight max abs diff: 0.00e+00\n",
      "[Check] Bias max abs diff: 0.00e+00\n",
      "[✓] LSTMFused 参数成功同步自 nn.LSTM。\n",
      "recurrent shape:  torch.Size([1, 32, 3, 32])\n",
      "bias shape:  torch.Size([1, 3, 32])\n",
      "Wx shape:  torch.Size([2, 8, 1, 3, 32])\n",
      "R :  tensor([[[[-0.9258, -0.5977, -0.0334,  ...,  1.5859, -0.7773,  0.5703],\n",
      "          [-1.0312,  0.0554,  0.1157,  ...,  0.5156, -0.7422,  0.1196],\n",
      "          [ 1.4922, -1.1562,  0.7930,  ...,  1.8750, -0.1572,  1.4922]],\n",
      "\n",
      "         [[-0.4258, -0.3281, -0.9727,  ...,  0.7383,  0.1846,  0.5547],\n",
      "          [-0.8906, -0.6758, -0.8398,  ..., -0.2520, -1.9375,  1.2500],\n",
      "          [-1.5703,  1.7109,  0.2129,  ...,  0.2891,  0.3633,  0.8203]],\n",
      "\n",
      "         [[-2.6406, -0.9102,  0.9570,  ..., -0.2578, -1.5703,  0.3066],\n",
      "          [-0.1914,  1.2969, -1.0078,  ...,  0.5117,  0.0160,  0.9844],\n",
      "          [ 0.3320,  0.4316,  0.8047,  ..., -0.7227,  1.6406,  0.7773]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8359,  0.1211,  0.5234,  ..., -2.1719,  0.5078, -1.2344],\n",
      "          [-1.3125,  0.8711,  0.0693,  ..., -0.6406,  0.5977,  1.4688],\n",
      "          [ 0.0315,  1.0703,  0.0074,  ..., -2.8281, -0.7969, -0.6602]],\n",
      "\n",
      "         [[ 1.3516,  0.4727,  1.1719,  ..., -1.0391, -1.5469, -1.2031],\n",
      "          [ 0.5312, -0.4844, -0.3086,  ...,  0.5039, -0.5586, -0.0562],\n",
      "          [ 0.1030,  1.0625, -0.9102,  ...,  0.2578, -0.2236,  2.3125]],\n",
      "\n",
      "         [[-0.2871, -1.0859, -0.9570,  ..., -0.9648,  0.1797, -0.1084],\n",
      "          [-0.7266,  0.1631, -0.0967,  ...,  0.0457, -1.5859,  0.0457],\n",
      "          [-1.0156,  0.9023, -0.0879,  ...,  0.1631, -0.4746,  0.4824]]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<PermuteBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/qinhaoping/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/qinhaoping/.cache/torch_extensions/py311_cu118/gru_HS32BS8NH1NS1DbDBbDRbDWbDGbDSbDAbNGR3NGW3NGI4NGT4SA0UDB1GRCV0GR/build.ninja...\n",
      "/mnt/second/qinhaoping/anaconda3/envs/flashrnn/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module gru_HS32BS8NH1NS1DbDBbDRbDWbDGbDSbDAbNGR3NGW3NGI4NGT4SA0UDB1GRCV0GR...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "out_my shape:  torch.Size([8, 2, 32])\n",
      "out_ref shape:  torch.Size([8, 2, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module gru_HS32BS8NH1NS1DbDBbDRbDWbDGbDSbDAbNGR3NGW3NGI4NGT4SA0UDB1GRCV0GR...\n"
     ]
    }
   ],
   "source": [
    "seq_len = 2\n",
    "\n",
    "\n",
    "# Models\n",
    "ref_lstm = nn.GRU(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    bias=True,\n",
    "    batch_first=True,\n",
    "    bidirectional=False,\n",
    ").to(device=device, dtype=dtype)\n",
    "my_lstm = GRUCuda(input_size, hidden_size, num_layers).to(\n",
    "    device=device, dtype=dtype\n",
    ")\n",
    "fused = False\n",
    "initialize_ref_lstm_constant(ref_lstm)\n",
    "# print_lstm_all_params(ref_lstm)\n",
    "sync_from_pytorch_lstm(my_lstm, ref_lstm, fused)  # 同步权重\n",
    "\n",
    "# Inputs\n",
    "# x = torch.randn(batch, seq_len, input_size, device=\"cuda\", requires_grad=False)\n",
    "x = torch.randn(\n",
    "    batch, seq_len, input_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "h0 = torch.zeros(\n",
    "    num_layers, batch, hidden_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "\n",
    "\n",
    "# Clone inputs for reference\n",
    "x_ref = x.detach().clone().requires_grad_()\n",
    "h0_ref = h0.detach().clone().requires_grad_()\n",
    "\n",
    "# Forward\n",
    "# out_ref: [B,T,H]\n",
    "# hn_ref \\ cn_ref: [num_layers,B,H]\n",
    "out_ref, hn_ref = ref_lstm(x_ref, (h0_ref))\n",
    "out_my, hn_my = my_lstm(x, (h0))\n",
    "# out_ref, (hn_ref, cn_ref) = ref_lstm(x_ref)\n",
    "# out_my, (hn_my, cn_my) = my_lstm(x)\n",
    "# print(\"out my: \", out_my)\n",
    "# print(\"out ref:\", out_ref)\n",
    "print(\"out_my shape: \", out_my.shape)\n",
    "print(\"out_ref shape: \", out_ref.shape)\n",
    "# Backward\n",
    "loss_my = out_my.sum()\n",
    "loss_ref = out_ref.sum()\n",
    "loss_my.backward()\n",
    "loss_ref.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2188, 0.2832, 0.2852, 0.1494, 0.2988, 0.2031, 0.2070, 0.1582,\n",
      "          0.1777, 0.3145, 0.3203, 0.3926, 0.2988, 0.1807, 0.2324, 0.1836,\n",
      "          0.2949, 0.3379, 0.1631, 0.4766, 0.1592, 0.2275, 0.2559, 0.2773,\n",
      "          0.1660, 0.2617, 0.1836, 0.2236, 0.2217, 0.3438, 0.2598, 0.2402],\n",
      "         [0.1953, 0.2441, 0.2314, 0.1484, 0.2471, 0.1846, 0.1904, 0.1650,\n",
      "          0.1738, 0.2461, 0.2598, 0.2910, 0.2402, 0.1768, 0.2051, 0.1719,\n",
      "          0.2490, 0.2793, 0.1562, 0.3242, 0.1543, 0.2109, 0.2178, 0.2314,\n",
      "          0.1729, 0.2236, 0.1768, 0.2002, 0.2051, 0.2637, 0.2285, 0.2188],\n",
      "         [0.0840, 0.0869, 0.0938, 0.0752, 0.0898, 0.0840, 0.0801, 0.0854,\n",
      "          0.0737, 0.0967, 0.0918, 0.1011, 0.1001, 0.0737, 0.0869, 0.0791,\n",
      "          0.0894, 0.0928, 0.0786, 0.1177, 0.0742, 0.0806, 0.0874, 0.0884,\n",
      "          0.0869, 0.0869, 0.0840, 0.0825, 0.0903, 0.0967, 0.0850, 0.0820],\n",
      "         [0.1953, 0.3301, 0.2471, 0.1157, 0.3281, 0.1572, 0.2021, 0.0718,\n",
      "          0.1914, 0.2793, 0.3672, 0.4023, 0.2119, 0.1982, 0.2021, 0.1572,\n",
      "          0.3418, 0.4102, 0.1211, 0.4043, 0.1396, 0.2598, 0.2490, 0.2891,\n",
      "          0.0752, 0.2715, 0.1260, 0.2217, 0.1475, 0.3496, 0.2969, 0.2773],\n",
      "         [0.1050, 0.1104, 0.1216, 0.0918, 0.1152, 0.1055, 0.0986, 0.1074,\n",
      "          0.0894, 0.1270, 0.1187, 0.1367, 0.1348, 0.0894, 0.1099, 0.0972,\n",
      "          0.1138, 0.1201, 0.0962, 0.1719, 0.0903, 0.0996, 0.1104, 0.1128,\n",
      "          0.1099, 0.1099, 0.1050, 0.1025, 0.1162, 0.1270, 0.1064, 0.1021],\n",
      "         [0.2578, 0.3320, 0.3574, 0.1797, 0.3672, 0.2354, 0.2412, 0.1367,\n",
      "          0.2070, 0.4121, 0.4004, 0.5352, 0.3613, 0.2090, 0.2676, 0.2168,\n",
      "          0.3574, 0.4160, 0.1973, 0.6602, 0.1924, 0.2578, 0.3125, 0.3398,\n",
      "          0.1289, 0.3164, 0.1914, 0.2637, 0.2119, 0.4551, 0.3027, 0.2734],\n",
      "         [0.1455, 0.1572, 0.1787, 0.1206, 0.1670, 0.1465, 0.1348, 0.1475,\n",
      "          0.1177, 0.1904, 0.1738, 0.2100, 0.2031, 0.1182, 0.1553, 0.1309,\n",
      "          0.1641, 0.1768, 0.1289, 0.2812, 0.1187, 0.1367, 0.1572, 0.1621,\n",
      "          0.1523, 0.1562, 0.1445, 0.1416, 0.1660, 0.1914, 0.1494, 0.1416],\n",
      "         [0.2559, 0.3066, 0.3574, 0.1875, 0.3438, 0.2422, 0.2344, 0.1709,\n",
      "          0.2031, 0.4062, 0.3691, 0.5078, 0.3926, 0.2031, 0.2715, 0.2178,\n",
      "          0.3320, 0.3828, 0.2051, 0.6836, 0.1953, 0.2441, 0.2988, 0.3203,\n",
      "          0.1689, 0.2988, 0.2109, 0.2539, 0.2451, 0.4316, 0.2832, 0.2578]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "print(hn_ref)\n",
    "print(hn_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2793, 0.1689, 0.2715, 0.3477, 0.1797, 0.2812, 0.1787, 0.3887,\n",
      "          0.2168, 0.2305, 0.2041, 0.1475, 0.2891, 0.1963, 0.3320, 0.2559,\n",
      "          0.2227, 0.1562, 0.2227, 0.2432, 0.3203, 0.1816, 0.2217, 0.2305,\n",
      "          0.4141, 0.2080, 0.3730, 0.2217, 0.3887, 0.2090, 0.1934, 0.1934],\n",
      "         [0.2305, 0.1650, 0.2246, 0.2656, 0.1699, 0.2295, 0.1729, 0.2812,\n",
      "          0.1982, 0.2002, 0.1934, 0.1436, 0.2324, 0.1846, 0.2656, 0.2188,\n",
      "          0.2129, 0.1572, 0.2100, 0.2100, 0.2539, 0.1738, 0.1963, 0.2051,\n",
      "          0.2988, 0.1885, 0.2793, 0.1973, 0.2988, 0.1885, 0.1836, 0.1846],\n",
      "         [0.0908, 0.0728, 0.0903, 0.0991, 0.0771, 0.0942, 0.0825, 0.1045,\n",
      "          0.0806, 0.0874, 0.0776, 0.0757, 0.0947, 0.0781, 0.0933, 0.0918,\n",
      "          0.0791, 0.0688, 0.0967, 0.0884, 0.0942, 0.0752, 0.0850, 0.0830,\n",
      "          0.1040, 0.0820, 0.0991, 0.0845, 0.0986, 0.0820, 0.0767, 0.0762],\n",
      "         [0.2715, 0.1738, 0.2500, 0.3418, 0.1660, 0.2314, 0.1260, 0.3574,\n",
      "          0.2285, 0.1865, 0.2227, 0.1099, 0.2393, 0.2031, 0.3750, 0.2080,\n",
      "          0.2617, 0.1680, 0.1025, 0.1963, 0.3320, 0.1875, 0.1924, 0.2334,\n",
      "          0.4277, 0.1914, 0.3965, 0.2002, 0.4492, 0.1875, 0.2041, 0.2109],\n",
      "         [0.1162, 0.0879, 0.1152, 0.1318, 0.0938, 0.1230, 0.1025, 0.1426,\n",
      "          0.0996, 0.1108, 0.0952, 0.0918, 0.1230, 0.0957, 0.1211, 0.1182,\n",
      "          0.0972, 0.0830, 0.1270, 0.1118, 0.1226, 0.0913, 0.1064, 0.1035,\n",
      "          0.1416, 0.1016, 0.1318, 0.1055, 0.1309, 0.1016, 0.0938, 0.0928],\n",
      "         [0.3496, 0.2012, 0.3379, 0.4648, 0.2129, 0.3457, 0.1895, 0.5391,\n",
      "          0.2500, 0.2773, 0.2324, 0.1846, 0.3672, 0.2275, 0.4180, 0.3008,\n",
      "          0.2471, 0.1885, 0.1592, 0.2891, 0.4102, 0.2129, 0.2598, 0.2715,\n",
      "          0.5781, 0.2441, 0.5039, 0.2598, 0.5078, 0.2461, 0.2236, 0.2217],\n",
      "         [0.1680, 0.1157, 0.1660, 0.1982, 0.1260, 0.1797, 0.1396, 0.2207,\n",
      "          0.1357, 0.1562, 0.1279, 0.1211, 0.1807, 0.1289, 0.1777, 0.1709,\n",
      "          0.1328, 0.1069, 0.1865, 0.1592, 0.1807, 0.1216, 0.1484, 0.1436,\n",
      "          0.2197, 0.1387, 0.2002, 0.1465, 0.1992, 0.1396, 0.1260, 0.1240],\n",
      "         [0.3340, 0.1982, 0.3281, 0.4492, 0.2119, 0.3516, 0.2070, 0.5312,\n",
      "          0.2402, 0.2793, 0.2236, 0.1914, 0.3691, 0.2217, 0.3867, 0.3105,\n",
      "          0.2344, 0.1875, 0.2256, 0.2910, 0.3887, 0.2080, 0.2598, 0.2598,\n",
      "          0.5508, 0.2412, 0.4727, 0.2578, 0.4727, 0.2432, 0.2168, 0.2139]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "print(hn_my)\n",
    "print(hn_my.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.279296875, 0.23046875, 0.0908203125, 0.271484375, 0.1162109375, 0.349609375, 0.16796875, 0.333984375]\n",
      "[0.21875, 0.1953125, 0.083984375, 0.1953125, 0.10498046875, 0.2578125, 0.1455078125, 0.255859375]\n"
     ]
    }
   ],
   "source": [
    "result1 = hn_my[0, :, 0].tolist()  # list of 16 elements\n",
    "result2 = hn_ref[0, :, 0].tolist()  # list of 16 elements\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Forward] Output max abs diff: 4.49e-01\n",
      "[Forward] hn     max abs diff: 4.49e-01\n",
      "[Forward] Output mean abs diff: 4.03e-02\n",
      "[Forward] hn     mean abs diff: 8.06e-02\n",
      "[Forward] Output max ref diff: 4.69e+00\n",
      "[Forward] hn     max ref diff: 4.69e+00\n",
      "[Forward] Output mean ref diff: 2.01e-01\n",
      "[Forward] hn     mean ref diff: 4.00e-01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Output comparison\n",
    "print(f\"[Forward] Output max abs diff: {max_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max abs diff: {max_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean abs diff: {mean_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean abs diff: {mean_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] Output max ref diff: {max_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max ref diff: {max_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean ref diff: {mean_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean ref diff: {mean_ref_diff(hn_my, hn_ref):.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradients\n",
    "print(f\"[Grad] Input x     grad diff: {max_abs_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {mean_abs_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {max_ref_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {mean_ref_diff(x.grad, x_ref.grad):.2e}\")\n",
    "\n",
    "print(f\"[Grad] h0          grad diff: {max_abs_diff(h0.grad, h0_ref.grad):.2e}\")\n",
    "\n",
    "for (n1, p1), (n2, p2) in zip(\n",
    "    my_lstm.named_parameters(), ref_lstm.named_parameters()\n",
    "):\n",
    "    if p1.grad is not None and p2.grad is not None:\n",
    "        diff = max_abs_diff(p1.grad, p2.grad)\n",
    "        print(f\"[Grad] Param {n1:20s} grad diff: {diff:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrnn.flashrnn import flashrnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_g: tensor([-0.9258, -0.4258, -2.6406,  ...,  0.2334, -0.6875,  1.0859],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)\n",
      "torch.Size([3, 1, 64, 64])\n",
      "tensor([[[[-0.9258, -0.4258, -2.6406,  ...,  0.1211,  0.4727, -1.0859],\n",
      "          [-0.0334, -0.9727,  0.9570,  ..., -0.2129, -0.3320, -0.2021],\n",
      "          [-1.1484, -0.5703, -0.6523,  ...,  0.2148, -0.7383, -0.4512],\n",
      "          ...,\n",
      "          [ 0.1680, -0.6953, -0.8633,  ...,  0.6797,  0.6445, -0.0197],\n",
      "          [-0.0153,  0.6875,  0.1709,  ..., -0.5312,  0.0552, -0.0664],\n",
      "          [ 0.6289,  0.9922,  0.9062,  ...,  1.6016, -1.0703,  1.6016]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2969,  0.1514,  2.0625,  ..., -0.8906,  1.1094,  0.5977],\n",
      "          [-0.4355,  0.4609,  0.3574,  ...,  0.1748, -1.0547, -1.2266],\n",
      "          [ 1.0000,  0.7227, -1.6250,  ..., -0.8008, -0.1719,  0.0952],\n",
      "          ...,\n",
      "          [-1.4219,  1.5703, -0.2891,  ...,  0.1226, -1.1875,  1.8359],\n",
      "          [-0.7891,  0.8711,  0.1660,  ...,  0.5742, -0.6367,  1.2969],\n",
      "          [ 0.8359, -0.3320,  1.6094,  ..., -0.4336, -0.8047, -0.0938]]],\n",
      "\n",
      "\n",
      "        [[[-0.8789,  0.7656,  1.5469,  ...,  0.0177,  0.2012, -0.5312],\n",
      "          [ 1.2500, -0.2031, -0.3516,  ..., -0.0874, -1.1172, -0.8125],\n",
      "          [ 1.3906, -1.4531,  0.0437,  ...,  0.2236,  1.0703,  0.4844],\n",
      "          ...,\n",
      "          [-0.4902, -1.7734, -0.1094,  ..., -0.7617,  1.0000,  1.2500],\n",
      "          [-1.4844,  1.0312, -1.0234,  ...,  0.1895, -1.3516, -2.2969],\n",
      "          [-1.2422, -1.7422,  0.4453,  ...,  0.2334, -0.6875,  1.0859]]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "torch.Size([16, 1024, 3, 1, 64])\n",
      "torch.Size([1, 64, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 1024\n",
    "DH = hidden_size // num_head\n",
    "NH=num_head\n",
    "\n",
    "gate_linear = torch.nn.Linear(input_size, num_gate * hidden_size).to(\n",
    "    device=device, dtype=dtype\n",
    ")\n",
    "with torch.no_grad():\n",
    "    gate_linear.weight.fill_(1.0)\n",
    "    if gate_linear.bias is not None:\n",
    "        gate_linear.bias.fill_(1.0)\n",
    "    # gate_linear.weight.fill_(0.0)\n",
    "    # if gate_linear.bias is not None:\n",
    "    #     gate_linear.bias.fill_(0.0)\n",
    "\n",
    "x = torch.randn(\n",
    "    batch, seq_len, input_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "# R=torch.cat([torch.ones((total_elems//2,),device=device,dtype=dtype),torch.full((total_elems//2,),2.0,device=device,dtype=dtype)],dim=0)\n",
    "# R.requires_grad_(requires_grad)\n",
    "print(\"R_g:\",R_g)\n",
    "R=R_g.view(num_gate, NH, DH, DH)\n",
    "print(R.shape)\n",
    "print(R)\n",
    "# R = torch.randn(\n",
    "#     [num_gate, NH, DH, DH],\n",
    "#     # [NH,DH,num_gate,DH],\n",
    "\n",
    "#     device=device,\n",
    "#     dtype=dtype,\n",
    "#     requires_grad=requires_grad,\n",
    "# ) \n",
    "\n",
    "b = torch.ones(\n",
    "    [num_gate, NH, DH],\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    requires_grad=requires_grad,\n",
    ")\n",
    "R_mtr = R.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "b_mtr = b.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "\n",
    "Wx = gate_linear(x)\n",
    "Wx = Wx.reshape(\n",
    "    Wx.shape[0], Wx.shape[1], R.shape[0], R.shape[1], R.shape[2]\n",
    ")\n",
    "# Wx = Wx.reshape(\n",
    "#     seq_len, batch, num_head, DH, num_gate\n",
    "# )\n",
    "Wx_mtr = Wx.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "\n",
    "\n",
    "print(Wx.shape)\n",
    "r=R\n",
    "r=r.permute(1,3,0,2)\n",
    "# print(Wx)\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref_lstm.weight_hh_l0.data: tensor([[-0.9258, -0.4258, -2.6406,  ...,  0.1211,  0.4727, -1.0859],\n",
      "        [-0.0334, -0.9727,  0.9570,  ..., -0.2129, -0.3320, -0.2021],\n",
      "        [-1.1484, -0.5703, -0.6523,  ...,  0.2148, -0.7383, -0.4512],\n",
      "        ...,\n",
      "        [-0.4902, -1.7734, -0.1094,  ..., -0.7617,  1.0000,  1.2500],\n",
      "        [-1.4844,  1.0312, -1.0234,  ...,  0.1895, -1.3516, -2.2969],\n",
      "        [-1.2422, -1.7422,  0.4453,  ...,  0.2334, -0.6875,  1.0859]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "function:  gru\n",
      "backend:  cuda\n",
      "Wx:  torch.Size([1024, 16, 1, 3, 64])\n",
      "R:  torch.Size([1, 64, 3, 64])\n",
      "b:  torch.Size([1, 3, 64])\n",
      "input states:  torch.Size([1, 16, 1, 1, 64])\n",
      "recurrent shape:  torch.Size([1, 64, 3, 64])\n",
      "bias shape:  torch.Size([1, 3, 64])\n",
      "Wx shape:  torch.Size([1024, 16, 1, 3, 64])\n",
      "R :  tensor([[[[-0.9258, -0.0334, -1.1484,  ...,  0.1680, -0.0153,  0.6289],\n",
      "          [ 2.2969, -0.4355,  1.0000,  ..., -1.4219, -0.7891,  0.8359],\n",
      "          [-0.8789,  1.2500,  1.3906,  ..., -0.4902, -1.4844, -1.2422]],\n",
      "\n",
      "         [[-0.4258, -0.9727, -0.5703,  ..., -0.6953,  0.6875,  0.9922],\n",
      "          [ 0.1514,  0.4609,  0.7227,  ...,  1.5703,  0.8711, -0.3320],\n",
      "          [ 0.7656, -0.2031, -1.4531,  ..., -1.7734,  1.0312, -1.7422]],\n",
      "\n",
      "         [[-2.6406,  0.9570, -0.6523,  ..., -0.8633,  0.1709,  0.9062],\n",
      "          [ 2.0625,  0.3574, -1.6250,  ..., -0.2891,  0.1660,  1.6094],\n",
      "          [ 1.5469, -0.3516,  0.0437,  ..., -0.1094, -1.0234,  0.4453]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1211, -0.2129,  0.2148,  ...,  0.6797, -0.5312,  1.6016],\n",
      "          [-0.8906,  0.1748, -0.8008,  ...,  0.1226,  0.5742, -0.4336],\n",
      "          [ 0.0177, -0.0874,  0.2236,  ..., -0.7617,  0.1895,  0.2334]],\n",
      "\n",
      "         [[ 0.4727, -0.3320, -0.7383,  ...,  0.6445,  0.0552, -1.0703],\n",
      "          [ 1.1094, -1.0547, -0.1719,  ..., -1.1875, -0.6367, -0.8047],\n",
      "          [ 0.2012, -1.1172,  1.0703,  ...,  1.0000, -1.3516, -0.6875]],\n",
      "\n",
      "         [[-1.0859, -0.2021, -0.4512,  ..., -0.0197, -0.0664,  1.6016],\n",
      "          [ 0.5977, -1.2266,  0.0952,  ...,  1.8359,  1.2969, -0.0938],\n",
      "          [-0.5312, -0.8125,  0.4844,  ...,  1.2500, -2.2969,  1.0859]]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<PermuteBackward0>)\n",
      "state:  torch.Size([1, 1025, 16, 1, 64])\n",
      "h:  torch.Size([1, 1024, 16, 1, 64])\n",
      "last_h:  torch.Size([1, 1, 16, 1, 64])\n",
      "output:  torch.Size([1, 1025, 16, 1, 64])\n",
      "out_my shape:  torch.Size([16, 1024, 64])\n",
      "out_ref shape:  torch.Size([16, 1024, 64])\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "ref_lstm = nn.GRU(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    bias=True,\n",
    "    batch_first=True,\n",
    "    bidirectional=False,\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "initialize_ref_lstm_constant(ref_lstm)\n",
    "\n",
    "# ========== 2. 同步 Recurrent 权重 R ==========\n",
    "# 转换成 [4H, H] 形式，用于赋值给 ref_lstm.weight_hh_l0\n",
    "# 步骤：\n",
    "R_perm = R         # [4, NH, D, D]\n",
    "R_reshaped = R_perm.reshape(3, hidden_size, hidden_size)  # [3, H, D]\n",
    "weight_hh = R_reshaped.reshape(3 * hidden_size, hidden_size)  # [3H, D]\n",
    "# 赋值到 ref_lstm 的 recurrent 权重\n",
    "ref_lstm.weight_hh_l0.data.copy_(weight_hh)\n",
    "print(\"ref_lstm.weight_hh_l0.data:\",ref_lstm.weight_hh_l0.data)\n",
    "\n",
    "\n",
    "# Inputs\n",
    "# x = torch.randn(batch, seq_len, input_size, device=\"cuda\", requires_grad=False)\n",
    "# h0 = torch.ones(\n",
    "#     num_layers, batch, hidden_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    "# )\n",
    "h0=torch.full((num_layers, batch, hidden_size),0,device=\"cuda\", requires_grad=True, dtype=dtype)\n",
    "\n",
    "\n",
    "# Clone inputs for reference\n",
    "x_ref = x.detach().clone().requires_grad_()\n",
    "h0_ref = h0.detach().clone().requires_grad_()\n",
    "h0=h0.reshape(1,batch,num_head,hidden_size)\n",
    "# Forward\n",
    "out_ref, hn_ref = ref_lstm(x_ref, h0_ref)      #\n",
    "# out_my, last_h = flashrnn(Wx,R,b, function=\"lstm\",backend=\"cuda_fused\",dtype=dtype_str)\n",
    "out_my, hn_my = flashrnn(Wx,R,b, states=h0[None, :],function=\"gru\",backend=\"cuda\",dtype=dtype_str)\n",
    "out_my=out_my.reshape(batch, seq_len, hidden_size)\n",
    "hn_my=hn_my.reshape(num_layers, batch, hidden_size)\n",
    "# cn_my=cn_my.reshape(num_layers, batch, hidden_size)\n",
    "\n",
    "# out_ref, (hn_ref, cn_ref) = ref_lstm(x_ref)\n",
    "# out_my, (hn_my, cn_my) = my_lstm(x)\n",
    "# print(\"out my: \", out_my)\n",
    "# print(\"out ref:\", out_ref)\n",
    "print(\"out_my shape: \", out_my.shape)    #  [NS, B,T,NH,D]\n",
    "print(\"out_ref shape: \", out_ref.shape)  # [B,T,H]\n",
    "# # Backward\n",
    "loss_my = out_my.sum()\n",
    "loss_ref = out_ref.sum()\n",
    "loss_my.backward()\n",
    "loss_ref.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0105, 0.0105, 0.0105,  ..., 0.0105, 0.0105, 0.0105],\n",
      "        [0.1680, 0.1680, 0.1680,  ..., 0.1680, 0.2695, 0.2656],\n",
      "        [0.0267, 0.0267, 0.0267,  ..., 0.0267, 0.0267, 0.0267],\n",
      "        ...,\n",
      "        [0.0596, 0.0596, 0.0596,  ..., 0.0596, 0.0601, 0.0601],\n",
      "        [0.1035, 0.1035, 0.1035,  ..., 0.1035, 0.1074, 0.1074],\n",
      "        [0.1719, 0.1719, 0.1719,  ..., 0.1719, 0.2539, 0.2520]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0105, 0.0105, 0.0105,  ..., 0.0105, 0.0105, 0.0105],\n",
      "        [0.1680, 0.1680, 0.1680,  ..., 0.1680, 0.1680, 0.1680],\n",
      "        [0.0265, 0.0265, 0.0265,  ..., 0.0265, 0.0265, 0.0265],\n",
      "        ...,\n",
      "        [0.0596, 0.0596, 0.0596,  ..., 0.0596, 0.0596, 0.0596],\n",
      "        [0.1035, 0.1035, 0.1035,  ..., 0.1035, 0.1035, 0.1035],\n",
      "        [0.1719, 0.1719, 0.1719,  ..., 0.1719, 0.1719, 0.1719]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out_my[:,0,:])\n",
    "print(out_ref[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 64])\n",
      "tensor([[[-0.5078,  0.1138,  1.0000,  ...,  0.4961,  1.0000, -0.7188],\n",
      "         [-0.4043,  0.7500,  0.9023,  ..., -0.4551,  1.0000,  0.3516],\n",
      "         [-0.7070, -0.5039,  1.0000,  ...,  0.4531,  1.0000, -0.2656],\n",
      "         ...,\n",
      "         [-0.4844, -0.2656,  0.9844,  ...,  0.6992,  1.0000, -0.1245],\n",
      "         [-0.8555, -0.1211,  0.7734,  ...,  0.7422,  1.0000, -0.7461],\n",
      "         [-0.6953,  0.4336,  0.9805,  ...,  0.7227,  1.0000, -0.0457]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.9727,  0.4199,  0.2227,  ..., -0.7422, -0.8086,  0.9570],\n",
      "         [ 0.8281,  0.1367,  1.0000,  ..., -0.0771, -0.7930,  0.3398],\n",
      "         [ 0.8555,  1.0000,  0.8516,  ...,  0.9727, -0.3027,  0.9922],\n",
      "         ...,\n",
      "         [-0.9180, -0.9961, -0.9961,  ..., -0.1216,  0.6641, -0.0640],\n",
      "         [-0.9062, -1.0000,  0.9688,  ...,  0.1904, -0.5703,  0.9531],\n",
      "         [ 0.1895, -1.0000,  0.5508,  ..., -0.9023, -0.8438,  0.3828]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hn_my=hn_my.reshape(num_layers, batch, hidden_size)\n",
    "print(hn_my.shape)\n",
    "\n",
    "print(hn_my)\n",
    "print(hn_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5078125, -0.404296875, -0.70703125, 0.099609375, -0.451171875, 0.05029296875, -0.5546875, 0.11572265625, -0.15234375, -0.96484375, -0.6953125, -0.2421875, -0.46875, -0.484375, -0.85546875, -0.6953125]\n",
      "[0.97265625, 0.828125, 0.85546875, 0.59375, 0.4921875, 0.7265625, 0.94140625, 0.0791015625, 0.7109375, 0.8203125, -0.7265625, 0.455078125, 0.81640625, -0.91796875, -0.90625, 0.189453125]\n"
     ]
    }
   ],
   "source": [
    "result1 = hn_my[0, :, 0].tolist()  # list of 16 elements\n",
    "result2 = hn_ref[0, :, 0].tolist()  # list of 16 elements\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Forward] Output max abs diff: 2.00e+00\n",
      "[Forward] hn     max abs diff: 2.00e+00\n",
      "[Forward] Output mean abs diff: 8.40e-01\n",
      "[Forward] hn     mean abs diff: 8.48e-01\n",
      "[Forward] Output max ref diff: 5.90e+05\n",
      "[Forward] hn     max ref diff: 3.72e+02\n",
      "[Forward] Output mean ref diff: 6.91e+00\n",
      "[Forward] hn     mean ref diff: 3.08e+00\n"
     ]
    }
   ],
   "source": [
    "# Output comparison\n",
    "print(f\"[Forward] Output max abs diff: {max_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max abs diff: {max_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "# print(f\"[Forward] cn     max abs diff: {max_abs_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean abs diff: {mean_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean abs diff: {mean_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "# print(f\"[Forward] cn     mean abs diff: {mean_abs_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output max ref diff: {max_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max ref diff: {max_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "# print(f\"[Forward] cn     max ref diff: {max_ref_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean ref diff: {mean_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean ref diff: {mean_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "# print(f\"[Forward] cn     mean ref diff: {mean_ref_diff(cn_my, cn_ref):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grad] Input x     grad diff: nan\n",
      "[Grad] Input x     grad diff: nan\n",
      "[Grad] Input x     grad diff: nan\n",
      "[Grad] Input x     grad diff: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3351573/3065796780.py:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538437738/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"[Grad] h0          grad diff: {max_abs_diff(h0[None, :].grad, h0_ref.grad):.2e}\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Grad] Input x     grad diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_ref_diff(x.grad,\u001b[38;5;250m \u001b[39mx_ref.grad)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Grad] Input x     grad diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_ref_diff(x.grad,\u001b[38;5;250m \u001b[39mx_ref.grad)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Grad] h0          grad diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmax_abs_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mh0_ref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (n1, p1), (n2, p2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m     10\u001b[39m     my_lstm.named_parameters(), ref_lstm.named_parameters()\n\u001b[32m     11\u001b[39m ):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p1.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m p2.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mmax_abs_diff\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmax_abs_diff\u001b[39m(a, b):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m).abs().max().item()\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'NoneType' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gradients\n",
    "print(f\"[Grad] Input x     grad diff: {max_abs_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {mean_abs_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {max_ref_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {mean_ref_diff(x.grad, x_ref.grad):.2e}\")\n",
    "\n",
    "print(f\"[Grad] h0          grad diff: {max_abs_diff(h0[None, :].grad, h0_ref.grad):.2e}\")\n",
    "\n",
    "for (n1, p1), (n2, p2) in zip(\n",
    "    my_lstm.named_parameters(), ref_lstm.named_parameters()\n",
    "):\n",
    "    if p1.grad is not None and p2.grad is not None:\n",
    "        diff = max_abs_diff(p1.grad, p2.grad)\n",
    "        print(f\"[Grad] Param {n1:20s} grad diff: {diff:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flashrnn)",
   "language": "python",
   "name": "flashrnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
