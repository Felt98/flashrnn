{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ build cuda fused ==============\n",
      "file path: /mnt/second/qinhaoping/repo/flashRNN/main/flashrnn/flashrnn\n",
      "file path: /mnt/second/qinhaoping/repo/flashRNN/main/flashrnn/flashrnn\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "debug diff\n",
    "R: all ones\n",
    "b: all ones\n",
    "x: randn \n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.append(os.path.abspath(os.path.join(__file__, \"../../..\")))\n",
    "\n",
    "\n",
    "from flashrnn.frameworks.cuda_alternating.gru import GRUCuda\n",
    "from flashrnn.frameworks.cuda_alternating.lstm import LSTMCuda\n",
    "from flashrnn.frameworks.cuda_fused.gru import GRUFused\n",
    "from flashrnn.frameworks.cuda_fused.lstm import LSTMFused\n",
    "from flashrnn.flashrnn import flashrnn\n",
    "from flashrnn.flashrnn_fused import FlashRNNFuncGeneratorFused\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "dtype_str = \"bfloat16\"\n",
    "###\n",
    "# Config\n",
    "input_size = 1\n",
    "hidden_size = 32\n",
    "batch = 8\n",
    "num_layers = 1\n",
    "num_head = 1\n",
    "num_gates = 4\n",
    "requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_ref_lstm_constant(ref_lstm:nn.LSTM, value=1.0):\n",
    "    with torch.no_grad():\n",
    "        for name, param in ref_lstm.named_parameters():\n",
    "            param.fill_(value)\n",
    "\n",
    "\n",
    "def sync_from_pytorch_lstm(my_lstm: LSTMFused, ref_lstm: nn.LSTM, fused: bool):\n",
    "    \"\"\"\n",
    "    同步 nn.LSTM 的第一层权重到自定义的 LSTMFused。\n",
    "    要求：\n",
    "    - my_lstm.num_heads == 1\n",
    "    - my_lstm.num_layers == 1\n",
    "    - ref_lstm.num_layers == 1，单向\n",
    "    \"\"\"\n",
    "    assert my_lstm.num_heads == 1, \"只能同步 num_heads == 1 的模型\"\n",
    "    assert my_lstm.num_layers == 1, \"只能同步单层模型\"\n",
    "    assert (\n",
    "        ref_lstm.num_layers == 1 and not ref_lstm.bidirectional\n",
    "    ), \"只支持同步单层单向 LSTM\"\n",
    "\n",
    "    H = my_lstm.hidden_size\n",
    "    I = my_lstm.linear.in_features  # 输入维度\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_lstm.bias_ih_l0.zero_()\n",
    "        ref_lstm.bias_hh_l0.zero_()\n",
    "        # ========== 1. 同步 Linear 权重 ==========\n",
    "        # ref: weight_ih_l0: [4H, I]\n",
    "        my_lstm.linear.weight.copy_(ref_lstm.weight_ih_l0)  # [4H, I]\n",
    "        my_lstm.linear.bias.copy_(ref_lstm.bias_ih_l0 + ref_lstm.bias_hh_l0)  # [4H]\n",
    "\n",
    "        # ========== 2. 同步 Recurrent 权重 R ==========\n",
    "        weight_hh = ref_lstm.weight_hh_l0  # shape [4H, H]\n",
    "        gates = torch.split(weight_hh, H, dim=0)  # 4 tensors of shape [H, H]\n",
    "        stacked = torch.stack(gates, dim=0)  # [4, H, H]\n",
    "        R = stacked.unsqueeze(0).permute(0, 2, 1, 3).contiguous()  # [1, H, 4, H]\n",
    "        my_lstm.recurrents[0].copy_(R)\n",
    "\n",
    "        # ========== 3. 同步 bias ==========\n",
    "        if fused:\n",
    "            total_bias = ref_lstm.bias_ih_l0 + ref_lstm.bias_hh_l0  # shape [4H]\n",
    "            gates_b = torch.split(total_bias, H, dim=0)  # 4 tensors of shape [H]\n",
    "            b_stacked = (\n",
    "                torch.stack(gates_b, dim=0).unsqueeze(0).permute(0, 2, 1)\n",
    "            )  # [1, H, 4]\n",
    "            my_lstm.biases[0].copy_(b_stacked)\n",
    "        else:\n",
    "            total_bias = ref_lstm.bias_ih_l0 + ref_lstm.bias_hh_l0  # shape [4H]\n",
    "            gates_b = torch.split(total_bias, H, dim=0)  # 4 tensors of shape [H]\n",
    "            b_stacked = (\n",
    "                torch.stack(gates_b, dim=0).unsqueeze(0).permute(0, 1, 2)\n",
    "            )  # [1, H, 4]\n",
    "            my_lstm.biases[0].copy_(b_stacked)\n",
    "\n",
    "        # ========== 验证是否同步成功 ==========\n",
    "        # [4H, I]\n",
    "        diff_w = (my_lstm.linear.weight - ref_lstm.weight_ih_l0).abs().max()\n",
    "        print(f\"[Check] Linear weight max abs diff: {diff_w:.2e}\")\n",
    "\n",
    "        # [4H]\n",
    "        expected_bias = ref_lstm.bias_ih_l0 + ref_lstm.bias_hh_l0\n",
    "        diff_b = (my_lstm.linear.bias - expected_bias).abs().max()\n",
    "        print(f\"[Check] Linear bias   max abs diff: {diff_b:.2e}\")\n",
    "\n",
    "        # [1, H, 4, H] -> [4, H, H]\n",
    "        R = my_lstm.recurrents[0].permute(2, 1, 3, 0).squeeze(3)  # [4, H, H]\n",
    "        R_flat = torch.cat([R[i] for i in range(4)], dim=0)  # [4H, H]\n",
    "        diff_R = (R_flat - ref_lstm.weight_hh_l0).abs().max()\n",
    "        print(f\"[Check] Recurrent weight max abs diff: {diff_R:.2e}\")\n",
    "\n",
    "        # [1, H, 4] -> [4H]\n",
    "        b_my = my_lstm.biases[0].permute(2, 1, 0).reshape(-1)  # [4H]\n",
    "        b_ref = ref_lstm.bias_ih_l0 + ref_lstm.bias_hh_l0\n",
    "        diff_bias = (b_my - b_ref).abs().max()\n",
    "        print(f\"[Check] Bias max abs diff: {diff_bias:.2e}\")\n",
    "    print(\"[✓] LSTMFused 参数成功同步自 nn.LSTM。\")\n",
    "\n",
    "\n",
    "def max_abs_diff(a, b):\n",
    "    return (a - b).abs().max().item()\n",
    "\n",
    "\n",
    "def mean_abs_diff(a, b):\n",
    "    return (a - b).abs().mean().item()\n",
    "\n",
    "\n",
    "def max_ref_diff(a, b, eps=1e-8):\n",
    "    return ((a - b).abs() / (b.abs() + eps)).max().item()\n",
    "\n",
    "\n",
    "def mean_ref_diff(a, b, eps=1e-8):\n",
    "    return ((a - b).abs() / (b.abs() + eps)).mean().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 4, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 2\n",
    "DH = hidden_size // num_head\n",
    "NH=num_head\n",
    "gate_linear = torch.nn.Linear(input_size, num_gates * hidden_size).to(\n",
    "    device=device, dtype=dtype\n",
    ")\n",
    "with torch.no_grad():\n",
    "    gate_linear.weight.fill_(1.0)\n",
    "    if gate_linear.bias is not None:\n",
    "        gate_linear.bias.fill_(1.0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "x = torch.randn(\n",
    "    batch, seq_len, input_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "\n",
    "R = torch.randn(\n",
    "    [num_gates, NH, DH, DH],\n",
    "    # [NH,DH,num_gates,DH],\n",
    "\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    requires_grad=requires_grad,\n",
    ") \n",
    "\n",
    "b = torch.ones(\n",
    "    [num_gates, NH, DH],\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    requires_grad=requires_grad,\n",
    ")\n",
    "R_mtr = R.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "b_mtr = b.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "\n",
    "Wx = gate_linear(x)\n",
    "Wx = Wx.reshape(\n",
    "    Wx.shape[0], Wx.shape[1], R.shape[0], R.shape[1], R.shape[2]\n",
    ")\n",
    "# Wx = Wx.reshape(\n",
    "#     seq_len, batch, num_head, DH, num_gates\n",
    "# )\n",
    "Wx_mtr = Wx.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "\n",
    "\n",
    "print(Wx.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function:  lstm\n",
      "backend:  cuda\n",
      "Wx:  torch.Size([2, 8, 1, 4, 32])\n",
      "R:  torch.Size([1, 32, 4, 32])\n",
      "b:  torch.Size([1, 4, 32])\n",
      "input states:  torch.Size([2, 1, 8, 1, 32])\n",
      "recurrent shape:  torch.Size([1, 32, 4, 32])\n",
      "bias shape:  torch.Size([1, 4, 32])\n",
      "Wx shape:  torch.Size([2, 8, 1, 4, 32])\n",
      "Wx :  tensor([[[[[ 0.0742,  0.0742,  0.0742,  ...,  0.0742,  0.0742,  0.0742],\n",
      "           [ 0.0742,  0.0742,  0.0742,  ...,  0.0742,  0.0742,  0.0742],\n",
      "           [ 0.0742,  0.0742,  0.0742,  ...,  0.0742,  0.0742,  0.0742],\n",
      "           [ 0.0742,  0.0742,  0.0742,  ...,  0.0742,  0.0742,  0.0742]]],\n",
      "\n",
      "\n",
      "         [[[-1.6406, -1.6406, -1.6406,  ..., -1.6406, -1.6406, -1.6406],\n",
      "           [-1.6406, -1.6406, -1.6406,  ..., -1.6406, -1.6406, -1.6406],\n",
      "           [-1.6406, -1.6406, -1.6406,  ..., -1.6406, -1.6406, -1.6406],\n",
      "           [-1.6406, -1.6406, -1.6406,  ..., -1.6406, -1.6406, -1.6406]]],\n",
      "\n",
      "\n",
      "         [[[ 0.8789,  0.8789,  0.8789,  ...,  0.8789,  0.8789,  0.8789],\n",
      "           [ 0.8789,  0.8789,  0.8789,  ...,  0.8789,  0.8789,  0.8789],\n",
      "           [ 0.8789,  0.8789,  0.8789,  ...,  0.8789,  0.8789,  0.8789],\n",
      "           [ 0.8789,  0.8789,  0.8789,  ...,  0.8789,  0.8789,  0.8789]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.6719, -0.6719, -0.6719,  ..., -0.6719, -0.6719, -0.6719],\n",
      "           [-0.6719, -0.6719, -0.6719,  ..., -0.6719, -0.6719, -0.6719],\n",
      "           [-0.6719, -0.6719, -0.6719,  ..., -0.6719, -0.6719, -0.6719],\n",
      "           [-0.6719, -0.6719, -0.6719,  ..., -0.6719, -0.6719, -0.6719]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3125,  1.3125,  1.3125,  ...,  1.3125,  1.3125,  1.3125],\n",
      "           [ 1.3125,  1.3125,  1.3125,  ...,  1.3125,  1.3125,  1.3125],\n",
      "           [ 1.3125,  1.3125,  1.3125,  ...,  1.3125,  1.3125,  1.3125],\n",
      "           [ 1.3125,  1.3125,  1.3125,  ...,  1.3125,  1.3125,  1.3125]]],\n",
      "\n",
      "\n",
      "         [[[ 2.2812,  2.2812,  2.2812,  ...,  2.2812,  2.2812,  2.2812],\n",
      "           [ 2.2812,  2.2812,  2.2812,  ...,  2.2812,  2.2812,  2.2812],\n",
      "           [ 2.2812,  2.2812,  2.2812,  ...,  2.2812,  2.2812,  2.2812],\n",
      "           [ 2.2812,  2.2812,  2.2812,  ...,  2.2812,  2.2812,  2.2812]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.5742,  0.5742,  0.5742,  ...,  0.5742,  0.5742,  0.5742],\n",
      "           [ 0.5742,  0.5742,  0.5742,  ...,  0.5742,  0.5742,  0.5742],\n",
      "           [ 0.5742,  0.5742,  0.5742,  ...,  0.5742,  0.5742,  0.5742],\n",
      "           [ 0.5742,  0.5742,  0.5742,  ...,  0.5742,  0.5742,  0.5742]]],\n",
      "\n",
      "\n",
      "         [[[ 1.1484,  1.1484,  1.1484,  ...,  1.1484,  1.1484,  1.1484],\n",
      "           [ 1.1484,  1.1484,  1.1484,  ...,  1.1484,  1.1484,  1.1484],\n",
      "           [ 1.1484,  1.1484,  1.1484,  ...,  1.1484,  1.1484,  1.1484],\n",
      "           [ 1.1484,  1.1484,  1.1484,  ...,  1.1484,  1.1484,  1.1484]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4219,  0.4219,  0.4219,  ...,  0.4219,  0.4219,  0.4219],\n",
      "           [ 0.4219,  0.4219,  0.4219,  ...,  0.4219,  0.4219,  0.4219],\n",
      "           [ 0.4219,  0.4219,  0.4219,  ...,  0.4219,  0.4219,  0.4219],\n",
      "           [ 0.4219,  0.4219,  0.4219,  ...,  0.4219,  0.4219,  0.4219]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 3.2656,  3.2656,  3.2656,  ...,  3.2656,  3.2656,  3.2656],\n",
      "           [ 3.2656,  3.2656,  3.2656,  ...,  3.2656,  3.2656,  3.2656],\n",
      "           [ 3.2656,  3.2656,  3.2656,  ...,  3.2656,  3.2656,  3.2656],\n",
      "           [ 3.2656,  3.2656,  3.2656,  ...,  3.2656,  3.2656,  3.2656]]],\n",
      "\n",
      "\n",
      "         [[[ 0.8164,  0.8164,  0.8164,  ...,  0.8164,  0.8164,  0.8164],\n",
      "           [ 0.8164,  0.8164,  0.8164,  ...,  0.8164,  0.8164,  0.8164],\n",
      "           [ 0.8164,  0.8164,  0.8164,  ...,  0.8164,  0.8164,  0.8164],\n",
      "           [ 0.8164,  0.8164,  0.8164,  ...,  0.8164,  0.8164,  0.8164]]],\n",
      "\n",
      "\n",
      "         [[[ 2.1875,  2.1875,  2.1875,  ...,  2.1875,  2.1875,  2.1875],\n",
      "           [ 2.1875,  2.1875,  2.1875,  ...,  2.1875,  2.1875,  2.1875],\n",
      "           [ 2.1875,  2.1875,  2.1875,  ...,  2.1875,  2.1875,  2.1875],\n",
      "           [ 2.1875,  2.1875,  2.1875,  ...,  2.1875,  2.1875,  2.1875]]]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<PermuteBackward0>)\n",
      "h:  torch.Size([2, 2, 8, 1, 32])\n",
      "last_h:  torch.Size([2, 1, 8, 1, 32])\n",
      "output:  torch.Size([2, 3, 8, 1, 32])\n",
      "h permute:  torch.Size([2, 8, 2, 1, 32])\n",
      "last_h permute:  torch.Size([2, 8, 1, 1, 32])\n",
      "out_my shape:  torch.Size([8, 2, 32])\n",
      "out_ref shape:  torch.Size([8, 2, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Models\n",
    "ref_lstm = nn.LSTM(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    bias=True,\n",
    "    batch_first=True,\n",
    "    bidirectional=False,\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "initialize_ref_lstm_constant(ref_lstm)\n",
    "\n",
    "# ========== 2. 同步 Recurrent 权重 R ==========\n",
    "# 转换成 [4H, H] 形式，用于赋值给 ref_lstm.weight_hh_l0\n",
    "# 步骤：\n",
    "R_perm = R         # [4, NH, D, D]\n",
    "R_reshaped = R_perm.reshape(4, hidden_size, hidden_size)  # [4, H, D]\n",
    "weight_hh = R_reshaped.reshape(4 * hidden_size, hidden_size)  # [4H, D]\n",
    "# 赋值到 ref_lstm 的 recurrent 权重\n",
    "ref_lstm.weight_hh_l0.data.copy_(weight_hh)\n",
    "\n",
    "\n",
    "\n",
    "# Inputs\n",
    "# x = torch.randn(batch, seq_len, input_size, device=\"cuda\", requires_grad=False)\n",
    "h0 = torch.zeros(\n",
    "    num_layers, batch, hidden_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "c0 = torch.zeros(\n",
    "    num_layers, batch, hidden_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "\n",
    "# Clone inputs for reference\n",
    "x_ref = x.detach().clone().requires_grad_()\n",
    "h0_ref = h0.detach().clone().requires_grad_()\n",
    "c0_ref = c0.detach().clone().requires_grad_()\n",
    "\n",
    "# Forward\n",
    "out_ref, (hn_ref, cn_ref) = ref_lstm(x_ref, (h0_ref, c0_ref))      #\n",
    "# out_my, last_h = flashrnn(Wx,R,b, function=\"lstm\",backend=\"cuda_fused\",dtype=dtype_str)\n",
    "out_my, (hn_my, cn_my) = flashrnn(Wx,R,b, function=\"lstm\",backend=\"cuda\",dtype=dtype_str)\n",
    "out_my=out_my.reshape(batch, seq_len, hidden_size)\n",
    "hn_my=hn_my.reshape(num_layers, batch, hidden_size)\n",
    "cn_my=cn_my.reshape(num_layers, batch, hidden_size)\n",
    "\n",
    "# out_ref, (hn_ref, cn_ref) = ref_lstm(x_ref)\n",
    "# out_my, (hn_my, cn_my) = my_lstm(x)\n",
    "# print(\"out my: \", out_my)\n",
    "# print(\"out ref:\", out_ref)\n",
    "print(\"out_my shape: \", out_my.shape)    #  [NS, B,T,NH,D]\n",
    "print(\"out_ref shape: \", out_ref.shape)  # [B,T,H]\n",
    "# # Backward\n",
    "# loss_my = out_my.sum()\n",
    "# loss_ref = out_ref.sum()\n",
    "# loss_my.backward()\n",
    "# loss_ref.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 32])\n",
      "tensor([[[ 7.4688e+00, -3.9375e+00,  7.2266e-01,  1.8750e+01, -3.6719e+00,\n",
      "           5.1562e+00,  1.0312e+00,  1.0000e+00,  1.8500e+01,  5.5312e+00,\n",
      "           2.5156e+00,  1.7250e+01,  1.8203e+00, -1.3516e+00,  6.3672e-01,\n",
      "           1.6484e+00, -1.0078e+00,  2.5977e-01,  5.7031e-01,  9.9609e-01,\n",
      "           1.0303e-01,  7.1875e-01,  8.9844e-02, -1.6484e+00,  6.9688e+00,\n",
      "           9.3384e-03,  5.2812e+00, -3.1562e+00, -1.0234e+00,  1.9531e+00,\n",
      "           1.5312e+00, -1.1094e+00],\n",
      "         [ 6.5625e+00, -1.0000e+01,  2.0410e-01,  2.7750e+01, -2.0750e+01,\n",
      "           1.0312e+01,  1.0078e+00,  8.8125e+00,  6.5234e-01,  1.1875e+01,\n",
      "           6.7500e+00,  2.1250e+01, -2.0752e-02,  1.5547e+00,  3.8125e+00,\n",
      "           3.9219e+00, -3.9844e+00,  1.6375e+01, -3.8750e+00,  1.6250e+00,\n",
      "           2.7954e-02,  2.1719e+00, -3.7500e-01, -5.3438e+00,  1.1188e+01,\n",
      "           2.1210e-03, -4.2188e+00, -2.0938e+00, -1.7734e+00,  3.5156e+00,\n",
      "           6.7969e-01,  3.4570e-01],\n",
      "         [ 2.2812e+00, -1.2750e+01,  2.1094e+00,  8.9375e+00, -7.6562e-01,\n",
      "           1.1938e+01,  1.0547e+00,  2.5000e+00,  2.1250e+00,  1.2062e+01,\n",
      "           8.8750e+00,  2.1750e+01,  2.0020e-02,  1.1438e+01,  3.0000e+00,\n",
      "           1.0750e+01, -9.8438e-01,  4.1875e+00, -1.0312e+01,  1.2812e+00,\n",
      "           6.7578e-01,  4.6562e+00, -2.6367e-01, -1.4219e+00,  1.5688e+01,\n",
      "          -1.3828e-05,  1.7875e+01, -3.3281e+00, -7.9688e-01,  6.5625e+00,\n",
      "           5.2734e-01,  1.3984e+00],\n",
      "         [ 7.1875e-01, -1.3516e+00,  2.1094e+00, -1.2062e+01,  3.7500e-01,\n",
      "           1.1250e+00,  1.9375e+00,  3.2344e+00, -1.1062e+01,  4.9219e-01,\n",
      "           3.5500e+01,  2.8750e+00,  3.3789e-01, -7.8125e+00,  3.1836e-01,\n",
      "           6.4062e-01,  1.2500e+00, -1.3594e+00,  1.3984e+00,  2.9531e+00,\n",
      "           1.6250e+01,  2.0875e+01,  2.3750e+01,  1.6875e+00,  1.2188e+00,\n",
      "           1.9750e+01,  2.4531e+00,  6.4062e+00,  2.0469e+00,  1.4219e+00,\n",
      "          -5.1500e+01,  2.5977e-01],\n",
      "         [ 2.6250e+00,  5.3750e+01,  4.8125e+00,  5.5938e+00,  3.2812e+00,\n",
      "           4.9375e+00, -1.2578e+00,  4.3164e-01,  2.1562e+00,  1.4297e+00,\n",
      "           6.6528e-03,  2.1406e+00,  2.0156e+00,  5.4688e-01, -1.1484e+00,\n",
      "           1.9531e+00, -8.8125e+00,  6.2109e-01,  6.8438e+00,  9.9375e+00,\n",
      "           8.8672e-01,  3.7969e+00, -3.6750e+01,  3.2344e+00,  2.8125e+01,\n",
      "          -1.0000e+00,  5.1172e-01,  1.3438e+01,  3.5781e+00,  1.0391e+00,\n",
      "           2.7344e+00,  2.2344e+00],\n",
      "         [ 4.8125e+00, -1.7188e-01, -7.0312e-02,  1.2500e+01, -1.0438e+01,\n",
      "           9.0000e+00,  2.6250e+00, -9.5703e-01,  1.3047e+00,  8.2500e+00,\n",
      "           5.4688e+00,  1.5000e+01,  1.0156e+00,  4.7188e+00, -5.1562e-01,\n",
      "           1.1641e+00, -7.3047e-01,  8.1250e+00,  1.1250e+01, -1.0376e-03,\n",
      "           4.2114e-03, -1.0078e+00,  1.6016e-01, -1.0250e+01, -1.3062e+01,\n",
      "          -1.1572e-01, -7.8125e+00, -4.2500e+00, -7.5312e+00, -1.7090e-01,\n",
      "           8.5449e-02,  3.5781e+00],\n",
      "         [ 7.2500e+00, -4.0312e+00,  1.9688e+00,  2.2875e+01, -1.9766e+00,\n",
      "           6.0000e+00,  1.0000e+00,  2.9062e+00,  2.6375e+01,  8.0000e+00,\n",
      "           7.4062e+00,  2.5000e+01,  3.3594e+00,  5.5625e+00,  2.6875e+00,\n",
      "           4.2812e+00, -1.2578e+00,  2.7656e+00, -4.0312e+00,  1.0156e+00,\n",
      "           4.9316e-02,  1.2578e+00,  2.6172e-01, -2.6094e+00,  9.3750e+00,\n",
      "          -5.8594e-03,  7.4375e+00, -5.2188e+00, -1.1797e+00,  4.9688e+00,\n",
      "           1.0625e+00,  8.6914e-02],\n",
      "         [ 2.8281e+00,  3.4500e+01,  3.6719e+00,  2.9375e+00,  9.6484e-01,\n",
      "           1.8281e+00, -1.1484e+00,  7.8125e-01,  2.7031e+00,  1.4375e+00,\n",
      "           1.7700e-02,  8.6875e+00,  9.9375e+00,  8.8281e-01, -1.3672e+00,\n",
      "           1.2938e+01, -9.1875e+00, -4.6484e-01,  4.3125e+00,  1.5812e+01,\n",
      "           9.2969e-01,  4.9688e+00, -1.2562e+01,  2.7812e+00,  2.1125e+01,\n",
      "          -9.6484e-01,  2.8594e+00,  5.3438e+00,  1.0000e+00,  8.9844e-01,\n",
      "           2.3926e-01,  7.4609e-01]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cn_my=cn_my.reshape(num_layers, batch, hidden_size)\n",
    "print(cn_my.shape)\n",
    "\n",
    "print(cn_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 32])\n",
      "tensor([[[ 7.4707e-02, -3.2031e-01, -5.7750e+01, -1.7188e+00, -1.9238e-01,\n",
      "           3.1250e-01, -6.1250e+00, -7.1094e-01,  9.9121e-02, -2.6123e-02,\n",
      "           4.3213e-02,  2.4658e-02,  1.3047e+00, -8.6426e-02, -5.3516e-01,\n",
      "          -6.4688e+00, -1.0376e-02,  3.3281e+00, -3.5156e-02, -9.0000e+00,\n",
      "          -2.3633e-01, -2.7832e-02, -5.0391e-01, -2.4805e-01, -1.4000e+01,\n",
      "           5.7422e-01,  3.8477e-01, -6.2561e-03, -9.2188e-01,  2.2461e-01,\n",
      "          -2.5757e-02,  9.7500e+00],\n",
      "         [ 6.7871e-02, -2.9883e-01, -5.7500e+01, -1.6953e+00, -1.9531e-01,\n",
      "           3.1250e-01, -6.5625e+00, -6.9922e-01,  8.9355e-02,  2.5146e-02,\n",
      "           4.7119e-02,  1.6602e-02,  1.3047e+00, -1.0449e-01, -5.0391e-01,\n",
      "          -6.6875e+00, -9.1553e-03,  3.2188e+00, -3.5400e-02, -9.7500e+00,\n",
      "          -2.1582e-01,  8.8501e-04, -5.1172e-01, -2.7539e-01, -1.3875e+01,\n",
      "           6.0547e-01,  3.6719e-01, -7.2327e-03, -8.6719e-01,  2.1680e-01,\n",
      "          -2.7222e-02,  9.6250e+00],\n",
      "         [ 5.9509e-03, -2.3633e-01, -5.7750e+01, -1.5781e+00, -2.0996e-01,\n",
      "           3.2227e-01, -6.4375e+00, -6.1719e-01,  1.3086e-01,  1.1133e-01,\n",
      "           6.0791e-02,  1.4221e-02,  1.4141e+00, -1.5137e-01, -5.0000e-01,\n",
      "          -7.0312e+00, -1.1169e-02,  3.0781e+00, -3.3691e-02, -9.2500e+00,\n",
      "          -7.5684e-02,  1.8311e-02, -5.3516e-01, -2.6172e-01, -1.3562e+01,\n",
      "           5.7031e-01,  2.1094e-01, -8.4229e-03, -7.8516e-01,  1.6992e-01,\n",
      "          -2.0752e-02,  9.8750e+00],\n",
      "         [ 2.4121e-01, -2.4023e-01, -5.7000e+01, -1.5703e+00, -2.0410e-01,\n",
      "           3.2422e-01, -6.4688e+00, -6.4062e-01,  8.7891e-02,  1.3379e-01,\n",
      "           2.7222e-02,  1.6846e-02,  1.3906e+00, -1.3477e-01, -4.9414e-01,\n",
      "          -7.0938e+00, -1.1658e-02,  3.0781e+00, -3.3691e-02, -9.5625e+00,\n",
      "          -8.7891e-02,  1.6235e-02, -5.6250e-01, -2.6172e-01, -1.3500e+01,\n",
      "           4.9414e-01,  3.4180e-01, -1.0071e-02, -7.9297e-01,  2.2754e-01,\n",
      "          -2.3315e-02,  9.7500e+00],\n",
      "         [ 1.5234e-01, -2.6367e-01, -5.7000e+01, -1.5391e+00, -2.0898e-01,\n",
      "           3.3594e-01, -6.1562e+00, -5.8203e-01,  1.5820e-01,  1.8652e-01,\n",
      "           3.4180e-02,  1.8433e-02,  1.5156e+00, -1.3379e-01, -5.6250e-01,\n",
      "          -7.3125e+00, -1.3184e-02,  3.0625e+00, -3.2715e-02, -8.8750e+00,\n",
      "          -5.0049e-02,  1.1414e-02, -5.4297e-01, -2.1191e-01, -1.3125e+01,\n",
      "           4.1016e-01,  3.1250e-01, -9.4604e-03, -7.6172e-01,  1.8359e-01,\n",
      "          -1.7334e-02,  9.8750e+00],\n",
      "         [ 1.0437e-02, -3.3789e-01, -5.7500e+01, -1.7031e+00, -1.9922e-01,\n",
      "           3.1445e-01, -6.4062e+00, -6.8750e-01,  1.2451e-01,  1.9409e-02,\n",
      "           4.1260e-02,  1.7578e-02,  1.3359e+00, -9.3262e-02, -5.4297e-01,\n",
      "          -6.8125e+00, -9.5825e-03,  3.2344e+00, -3.4668e-02, -9.1875e+00,\n",
      "          -2.1973e-01, -1.5335e-03, -4.9609e-01, -2.3828e-01, -1.3688e+01,\n",
      "           5.6641e-01,  3.9453e-01, -6.4392e-03, -8.6719e-01,  1.9629e-01,\n",
      "          -2.4170e-02,  9.8125e+00],\n",
      "         [ 1.0254e-01, -2.9102e-01, -5.7500e+01, -1.6562e+00, -1.9336e-01,\n",
      "           3.1641e-01, -6.4688e+00, -7.0312e-01,  1.0107e-01,  2.5757e-02,\n",
      "           4.2969e-02,  1.8799e-02,  1.3203e+00, -1.0742e-01, -5.0391e-01,\n",
      "          -6.7188e+00, -1.0071e-02,  3.2344e+00, -3.5400e-02, -9.3125e+00,\n",
      "          -1.9824e-01,  1.3173e-05, -5.1172e-01, -2.7734e-01, -1.3875e+01,\n",
      "           5.8594e-01,  3.5938e-01, -7.3242e-03, -8.7109e-01,  2.1680e-01,\n",
      "          -2.5757e-02,  9.8125e+00],\n",
      "         [ 3.5645e-02, -3.6133e-01, -5.9000e+01, -1.8203e+00, -1.9531e-01,\n",
      "           3.1250e-01, -6.6250e+00, -7.0312e-01,  5.2246e-02, -7.0312e-02,\n",
      "           5.1025e-02,  2.1240e-02,  1.2812e+00, -8.5449e-02, -5.1953e-01,\n",
      "          -6.7188e+00, -1.0132e-02,  3.2969e+00, -3.3691e-02, -8.7500e+00,\n",
      "          -2.7148e-01,  3.3203e-02, -4.9414e-01, -2.9883e-01, -1.3938e+01,\n",
      "           5.8984e-01,  4.2773e-01, -6.2561e-03, -8.4766e-01,  2.2656e-01,\n",
      "          -2.8442e-02,  9.5000e+00]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(cn_ref.shape)\n",
    "\n",
    "print(cn_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.46875, 6.5625, 2.28125, 0.71875, 2.625, 4.8125, 7.25, 2.828125]\n",
      "[0.07470703125, 0.06787109375, 0.005950927734375, 0.2412109375, 0.15234375, 0.01043701171875, 0.1025390625, 0.03564453125]\n"
     ]
    }
   ],
   "source": [
    "result1 = cn_my[0, :, 0].tolist()  # list of 16 elements\n",
    "result2 = cn_ref[0, :, 0].tolist()  # list of 16 elements\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Forward] Output max abs diff: 1.98e+00\n",
      "[Forward] hn     max abs diff: 1.84e+00\n",
      "[Forward] cn     max abs diff: 6.28e+01\n",
      "[Forward] Output mean abs diff: 6.13e-01\n",
      "[Forward] hn     mean abs diff: 7.03e-01\n",
      "[Forward] cn     mean abs diff: 8.69e+00\n",
      "[Forward] Output max ref diff: 7.83e+06\n",
      "[Forward] hn     max ref diff: 1.71e+05\n",
      "[Forward] cn     max ref diff: 9.52e+04\n",
      "[Forward] Output mean ref diff: 1.59e+02\n",
      "[Forward] hn     mean ref diff: 7.68e+02\n",
      "[Forward] cn     mean ref diff: 4.92e+02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Output comparison\n",
    "print(f\"[Forward] Output max abs diff: {max_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max abs diff: {max_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     max abs diff: {max_abs_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean abs diff: {mean_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean abs diff: {mean_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     mean abs diff: {mean_abs_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output max ref diff: {max_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max ref diff: {max_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     max ref diff: {max_ref_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean ref diff: {mean_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean ref diff: {mean_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     mean ref diff: {mean_ref_diff(cn_my, cn_ref):.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradients\n",
    "print(f\"[Grad] Input x     grad diff: {max_abs_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {mean_abs_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {max_ref_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {mean_ref_diff(x.grad, x_ref.grad):.2e}\")\n",
    "\n",
    "print(f\"[Grad] h0          grad diff: {max_abs_diff(h0.grad, h0_ref.grad):.2e}\")\n",
    "print(f\"[Grad] c0          grad diff: {max_abs_diff(c0.grad, c0_ref.grad):.2e}\")\n",
    "\n",
    "for (n1, p1), (n2, p2) in zip(\n",
    "    my_lstm.named_parameters(), ref_lstm.named_parameters()\n",
    "):\n",
    "    if p1.grad is not None and p2.grad is not None:\n",
    "        diff = max_abs_diff(p1.grad, p2.grad)\n",
    "        print(f\"[Grad] Param {n1:20s} grad diff: {diff:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flashrnn)",
   "language": "python",
   "name": "flashrnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
