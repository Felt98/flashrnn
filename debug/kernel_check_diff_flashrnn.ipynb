{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ build cuda fused ==============\n",
      "file path: /mnt/second/qinhaoping/repo/flashRNN/main/flashrnn/flashrnn\n",
      "file path: /mnt/second/qinhaoping/repo/flashRNN/main/flashrnn/flashrnn\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "debug diff\n",
    "R: all ones\n",
    "b: all ones\n",
    "x: randn \n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.append(os.path.abspath(os.path.join(__file__, \"../../..\")))\n",
    "\n",
    "\n",
    "from flashrnn.frameworks.cuda_alternating.gru import GRUCuda\n",
    "from flashrnn.frameworks.cuda_alternating.lstm import LSTMCuda\n",
    "from flashrnn.frameworks.cuda_fused.gru import GRUFused\n",
    "from flashrnn.frameworks.cuda_fused.lstm import LSTMFused\n",
    "from flashrnn.flashrnn import flashrnn\n",
    "from flashrnn.flashrnn_fused import FlashRNNFuncGeneratorFused\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "dtype_str = \"bfloat16\"\n",
    "###\n",
    "# Config\n",
    "input_size = 1\n",
    "hidden_size = 32\n",
    "batch = 8\n",
    "num_layers = 1\n",
    "num_head = 1\n",
    "num_gates = 4\n",
    "requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_ref_lstm_constant(ref_lstm:nn.LSTM, value=1.0):\n",
    "    with torch.no_grad():\n",
    "        for name, param in ref_lstm.named_parameters():\n",
    "            param.fill_(value)\n",
    "\n",
    "\n",
    "def sync_from_pytorch_lstm(my_lstm: LSTMFused, ref_lstm: nn.LSTM, fused: bool):\n",
    "    \"\"\"\n",
    "    同步 nn.LSTM 的第一层权重到自定义的 LSTMFused。\n",
    "    要求：\n",
    "    - my_lstm.num_heads == 1\n",
    "    - my_lstm.num_layers == 1\n",
    "    - ref_lstm.num_layers == 1，单向\n",
    "    \"\"\"\n",
    "    assert my_lstm.num_heads == 1, \"只能同步 num_heads == 1 的模型\"\n",
    "    assert my_lstm.num_layers == 1, \"只能同步单层模型\"\n",
    "    assert (\n",
    "        ref_lstm.num_layers == 1 and not ref_lstm.bidirectional\n",
    "    ), \"只支持同步单层单向 LSTM\"\n",
    "\n",
    "    H = my_lstm.hidden_size\n",
    "    I = my_lstm.linear.in_features  # 输入维度\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_lstm.bias_ih_l0.zero_()\n",
    "        ref_lstm.bias_hh_l0.zero_()\n",
    "        # ========== 1. 同步 Linear 权重 ==========\n",
    "        # ref: weight_ih_l0: [4H, I]\n",
    "        my_lstm.linear.weight.copy_(ref_lstm.weight_ih_l0)  # [4H, I]\n",
    "        my_lstm.linear.bias.copy_(ref_lstm.bias_ih_l0 + ref_lstm.bias_hh_l0)  # [4H]\n",
    "\n",
    "        # ========== 2. 同步 Recurrent 权重 R ==========\n",
    "        weight_hh = ref_lstm.weight_hh_l0  # shape [4H, H]\n",
    "        gates = torch.split(weight_hh, H, dim=0)  # 4 tensors of shape [H, H]\n",
    "        stacked = torch.stack(gates, dim=0)  # [4, H, H]\n",
    "        R = stacked.unsqueeze(0).permute(0, 2, 1, 3).contiguous()  # [1, H, 4, H]\n",
    "        my_lstm.recurrents[0].copy_(R)\n",
    "\n",
    "        # ========== 3. 同步 bias ==========\n",
    "        if fused:\n",
    "            total_bias = ref_lstm.bias_ih_l0 + ref_lstm.bias_hh_l0  # shape [4H]\n",
    "            gates_b = torch.split(total_bias, H, dim=0)  # 4 tensors of shape [H]\n",
    "            b_stacked = (\n",
    "                torch.stack(gates_b, dim=0).unsqueeze(0).permute(0, 2, 1)\n",
    "            )  # [1, H, 4]\n",
    "            my_lstm.biases[0].copy_(b_stacked)\n",
    "        else:\n",
    "            total_bias = ref_lstm.bias_ih_l0 + ref_lstm.bias_hh_l0  # shape [4H]\n",
    "            gates_b = torch.split(total_bias, H, dim=0)  # 4 tensors of shape [H]\n",
    "            b_stacked = (\n",
    "                torch.stack(gates_b, dim=0).unsqueeze(0).permute(0, 1, 2)\n",
    "            )  # [1, H, 4]\n",
    "            my_lstm.biases[0].copy_(b_stacked)\n",
    "\n",
    "        # ========== 验证是否同步成功 ==========\n",
    "        # [4H, I]\n",
    "        diff_w = (my_lstm.linear.weight - ref_lstm.weight_ih_l0).abs().max()\n",
    "        print(f\"[Check] Linear weight max abs diff: {diff_w:.2e}\")\n",
    "\n",
    "        # [4H]\n",
    "        expected_bias = ref_lstm.bias_ih_l0 + ref_lstm.bias_hh_l0\n",
    "        diff_b = (my_lstm.linear.bias - expected_bias).abs().max()\n",
    "        print(f\"[Check] Linear bias   max abs diff: {diff_b:.2e}\")\n",
    "\n",
    "        # [1, H, 4, H] -> [4, H, H]\n",
    "        R = my_lstm.recurrents[0].permute(2, 1, 3, 0).squeeze(3)  # [4, H, H]\n",
    "        R_flat = torch.cat([R[i] for i in range(4)], dim=0)  # [4H, H]\n",
    "        diff_R = (R_flat - ref_lstm.weight_hh_l0).abs().max()\n",
    "        print(f\"[Check] Recurrent weight max abs diff: {diff_R:.2e}\")\n",
    "\n",
    "        # [1, H, 4] -> [4H]\n",
    "        b_my = my_lstm.biases[0].permute(2, 1, 0).reshape(-1)  # [4H]\n",
    "        b_ref = ref_lstm.bias_ih_l0 + ref_lstm.bias_hh_l0\n",
    "        diff_bias = (b_my - b_ref).abs().max()\n",
    "        print(f\"[Check] Bias max abs diff: {diff_bias:.2e}\")\n",
    "    print(\"[✓] LSTMFused 参数成功同步自 nn.LSTM。\")\n",
    "\n",
    "\n",
    "def max_abs_diff(a, b):\n",
    "    return (a - b).abs().max().item()\n",
    "\n",
    "\n",
    "def mean_abs_diff(a, b):\n",
    "    return (a - b).abs().mean().item()\n",
    "\n",
    "\n",
    "def max_ref_diff(a, b, eps=1e-8):\n",
    "    return ((a - b).abs() / (b.abs() + eps)).max().item()\n",
    "\n",
    "\n",
    "def mean_ref_diff(a, b, eps=1e-8):\n",
    "    return ((a - b).abs() / (b.abs() + eps)).mean().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 32, 32])\n",
      "tensor([[[[ 2.5469, -0.7148, -0.4941,  ..., -0.6289, -0.6602,  2.0781],\n",
      "          [ 1.4141, -0.3086, -0.2051,  ...,  1.6016,  0.1328,  1.0703],\n",
      "          [-1.1172, -0.8398, -3.6719,  ..., -1.0234, -0.3301, -0.8633],\n",
      "          ...,\n",
      "          [-0.6289, -0.2295, -0.5430,  ...,  1.0391,  0.9609, -0.7891],\n",
      "          [-1.6016, -0.7617, -1.4531,  ...,  0.7227, -0.1670, -0.4785],\n",
      "          [-0.2812,  0.7109,  0.3965,  ..., -0.3906,  1.4766, -1.3281]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -0.9141,  0.9219,  ...,  0.9531, -0.2910, -1.3203],\n",
      "          [ 0.5625,  0.9375, -0.6953,  ...,  0.4863, -1.6406, -2.7500],\n",
      "          [ 0.4004,  1.4844,  0.9141,  ..., -0.6406, -0.7695, -1.3594],\n",
      "          ...,\n",
      "          [-1.3047,  0.7656,  0.7266,  ..., -1.2578, -2.4062,  0.2969],\n",
      "          [-1.3047, -1.9141,  0.3203,  ..., -1.1406, -1.1328, -0.0272],\n",
      "          [-1.1875, -0.7266, -0.8789,  ..., -0.5039, -1.1953,  0.2295]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6016,  0.2891,  1.9844,  ...,  0.0254, -1.5000,  1.1641],\n",
      "          [ 0.8359, -1.9453,  1.4375,  ..., -1.0859, -0.3887, -0.0923],\n",
      "          [ 0.3516,  0.4824,  0.5586,  ..., -0.0854, -0.0518,  2.0469],\n",
      "          ...,\n",
      "          [ 1.4453, -1.0156,  0.2422,  ...,  0.1367, -0.0113,  0.0630],\n",
      "          [ 1.0469,  0.4473,  0.3398,  ...,  0.1494, -1.6562, -1.0625],\n",
      "          [ 1.3359, -1.1484, -0.9727,  ..., -0.0996, -0.5195,  1.1484]]],\n",
      "\n",
      "\n",
      "        [[[-0.3203, -0.1465, -0.0835,  ..., -0.2520, -0.6797, -0.4746],\n",
      "          [ 0.4316,  1.2266, -0.0771,  ..., -1.3125,  0.7969,  0.6211],\n",
      "          [ 0.6016, -0.1875,  0.7500,  ..., -0.4043,  2.3125,  0.4922],\n",
      "          ...,\n",
      "          [-0.4863,  0.7070, -0.4023,  ..., -0.1865,  0.7188,  0.0923],\n",
      "          [-0.5508,  2.4062,  0.4395,  ...,  0.2490, -1.7109, -2.6406],\n",
      "          [-0.4102,  1.1406, -0.0374,  ..., -1.6250, -0.4121, -0.0630]]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "torch.Size([8, 2, 4, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 2\n",
    "DH = hidden_size // num_head\n",
    "NH=num_head\n",
    "\n",
    "gate_linear = torch.nn.Linear(input_size, num_gates * hidden_size).to(\n",
    "    device=device, dtype=dtype\n",
    ")\n",
    "with torch.no_grad():\n",
    "    gate_linear.weight.fill_(1.0)\n",
    "    if gate_linear.bias is not None:\n",
    "        gate_linear.bias.fill_(1.0)\n",
    "\n",
    "x = torch.randn(\n",
    "    batch, seq_len, input_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "total_elems = 4*NH*DH*DH\n",
    "# R=torch.cat([torch.ones((total_elems//2,),device=device,dtype=dtype),torch.full((total_elems//2,),2.0,device=device,dtype=dtype)],dim=0)\n",
    "# R.requires_grad_(requires_grad)\n",
    "R = torch.randn(total_elems, device=device,\n",
    "    dtype=dtype,\n",
    "    requires_grad=requires_grad,)  # 随机初始化一次\n",
    "R=R.view(num_gates, NH, DH, DH)\n",
    "print(R.shape)\n",
    "print(R)\n",
    "# R = torch.randn(\n",
    "#     [num_gates, NH, DH, DH],\n",
    "#     # [NH,DH,num_gates,DH],\n",
    "\n",
    "#     device=device,\n",
    "#     dtype=dtype,\n",
    "#     requires_grad=requires_grad,\n",
    "# ) \n",
    "\n",
    "b = torch.ones(\n",
    "    [num_gates, NH, DH],\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    requires_grad=requires_grad,\n",
    ")\n",
    "R_mtr = R.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "b_mtr = b.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "\n",
    "Wx = gate_linear(x)\n",
    "Wx = Wx.reshape(\n",
    "    Wx.shape[0], Wx.shape[1], R.shape[0], R.shape[1], R.shape[2]\n",
    ")\n",
    "# Wx = Wx.reshape(\n",
    "#     seq_len, batch, num_head, DH, num_gates\n",
    "# )\n",
    "Wx_mtr = Wx.clone().to(dtype=dtype).detach().requires_grad_(requires_grad)\n",
    "\n",
    "\n",
    "print(Wx.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref_lstm.weight_hh_l0.data: tensor([[ 2.5469, -0.7148, -0.4941,  ..., -0.6289, -0.6602,  2.0781],\n",
      "        [ 1.4141, -0.3086, -0.2051,  ...,  1.6016,  0.1328,  1.0703],\n",
      "        [-1.1172, -0.8398, -3.6719,  ..., -1.0234, -0.3301, -0.8633],\n",
      "        ...,\n",
      "        [-0.4863,  0.7070, -0.4023,  ..., -0.1865,  0.7188,  0.0923],\n",
      "        [-0.5508,  2.4062,  0.4395,  ...,  0.2490, -1.7109, -2.6406],\n",
      "        [-0.4102,  1.1406, -0.0374,  ..., -1.6250, -0.4121, -0.0630]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "function:  lstm\n",
      "backend:  cuda\n",
      "Wx:  torch.Size([2, 8, 1, 4, 32])\n",
      "R:  torch.Size([1, 32, 4, 32])\n",
      "b:  torch.Size([1, 4, 32])\n",
      "input states:  torch.Size([2, 1, 8, 1, 32])\n",
      "recurrent shape:  torch.Size([1, 32, 4, 32])\n",
      "bias shape:  torch.Size([1, 4, 32])\n",
      "Wx shape:  torch.Size([2, 8, 1, 4, 32])\n",
      "R :  tensor([[[[ 2.5469,  1.4141, -1.1172,  ..., -0.6289, -1.6016, -0.2812],\n",
      "          [-1.0000,  0.5625,  0.4004,  ..., -1.3047, -1.3047, -1.1875],\n",
      "          [ 1.6016,  0.8359,  0.3516,  ...,  1.4453,  1.0469,  1.3359],\n",
      "          [-0.3203,  0.4316,  0.6016,  ..., -0.4863, -0.5508, -0.4102]],\n",
      "\n",
      "         [[-0.7148, -0.3086, -0.8398,  ..., -0.2295, -0.7617,  0.7109],\n",
      "          [-0.9141,  0.9375,  1.4844,  ...,  0.7656, -1.9141, -0.7266],\n",
      "          [ 0.2891, -1.9453,  0.4824,  ..., -1.0156,  0.4473, -1.1484],\n",
      "          [-0.1465,  1.2266, -0.1875,  ...,  0.7070,  2.4062,  1.1406]],\n",
      "\n",
      "         [[-0.4941, -0.2051, -3.6719,  ..., -0.5430, -1.4531,  0.3965],\n",
      "          [ 0.9219, -0.6953,  0.9141,  ...,  0.7266,  0.3203, -0.8789],\n",
      "          [ 1.9844,  1.4375,  0.5586,  ...,  0.2422,  0.3398, -0.9727],\n",
      "          [-0.0835, -0.0771,  0.7500,  ..., -0.4023,  0.4395, -0.0374]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6289,  1.6016, -1.0234,  ...,  1.0391,  0.7227, -0.3906],\n",
      "          [ 0.9531,  0.4863, -0.6406,  ..., -1.2578, -1.1406, -0.5039],\n",
      "          [ 0.0254, -1.0859, -0.0854,  ...,  0.1367,  0.1494, -0.0996],\n",
      "          [-0.2520, -1.3125, -0.4043,  ..., -0.1865,  0.2490, -1.6250]],\n",
      "\n",
      "         [[-0.6602,  0.1328, -0.3301,  ...,  0.9609, -0.1670,  1.4766],\n",
      "          [-0.2910, -1.6406, -0.7695,  ..., -2.4062, -1.1328, -1.1953],\n",
      "          [-1.5000, -0.3887, -0.0518,  ..., -0.0113, -1.6562, -0.5195],\n",
      "          [-0.6797,  0.7969,  2.3125,  ...,  0.7188, -1.7109, -0.4121]],\n",
      "\n",
      "         [[ 2.0781,  1.0703, -0.8633,  ..., -0.7891, -0.4785, -1.3281],\n",
      "          [-1.3203, -2.7500, -1.3594,  ...,  0.2969, -0.0272,  0.2295],\n",
      "          [ 1.1641, -0.0923,  2.0469,  ...,  0.0630, -1.0625,  1.1484],\n",
      "          [-0.4746,  0.6211,  0.4922,  ...,  0.0923, -2.6406, -0.0630]]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<PermuteBackward0>)\n",
      "state:  torch.Size([2, 3, 8, 1, 32])\n",
      "h:  torch.Size([2, 2, 8, 1, 32])\n",
      "last_h:  torch.Size([2, 1, 8, 1, 32])\n",
      "output:  torch.Size([2, 3, 8, 1, 32])\n",
      "out_my shape:  torch.Size([8, 2, 32])\n",
      "out_ref shape:  torch.Size([8, 2, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Models\n",
    "ref_lstm = nn.LSTM(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    bias=True,\n",
    "    batch_first=True,\n",
    "    bidirectional=False,\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "initialize_ref_lstm_constant(ref_lstm)\n",
    "\n",
    "# ========== 2. 同步 Recurrent 权重 R ==========\n",
    "# 转换成 [4H, H] 形式，用于赋值给 ref_lstm.weight_hh_l0\n",
    "# 步骤：\n",
    "R_perm = R         # [4, NH, D, D]\n",
    "R_reshaped = R_perm.reshape(4, hidden_size, hidden_size)  # [4, H, D]\n",
    "weight_hh = R_reshaped.reshape(4 * hidden_size, hidden_size)  # [4H, D]\n",
    "# 赋值到 ref_lstm 的 recurrent 权重\n",
    "ref_lstm.weight_hh_l0.data.copy_(weight_hh)\n",
    "print(\"ref_lstm.weight_hh_l0.data:\",ref_lstm.weight_hh_l0.data)\n",
    "\n",
    "\n",
    "# Inputs\n",
    "# x = torch.randn(batch, seq_len, input_size, device=\"cuda\", requires_grad=False)\n",
    "h0 = torch.zeros(\n",
    "    num_layers, batch, hidden_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "c0 = torch.zeros(\n",
    "    num_layers, batch, hidden_size, device=\"cuda\", requires_grad=True, dtype=dtype\n",
    ")\n",
    "\n",
    "# Clone inputs for reference\n",
    "x_ref = x.detach().clone().requires_grad_()\n",
    "h0_ref = h0.detach().clone().requires_grad_()\n",
    "c0_ref = c0.detach().clone().requires_grad_()\n",
    "\n",
    "# Forward\n",
    "out_ref, (hn_ref, cn_ref) = ref_lstm(x_ref, (h0_ref, c0_ref))      #\n",
    "# out_my, last_h = flashrnn(Wx,R,b, function=\"lstm\",backend=\"cuda_fused\",dtype=dtype_str)\n",
    "out_my, (hn_my, cn_my) = flashrnn(Wx,R,b, function=\"lstm\",backend=\"cuda\",dtype=dtype_str)\n",
    "out_my=out_my.reshape(batch, seq_len, hidden_size)\n",
    "hn_my=hn_my.reshape(num_layers, batch, hidden_size)\n",
    "cn_my=cn_my.reshape(num_layers, batch, hidden_size)\n",
    "\n",
    "# out_ref, (hn_ref, cn_ref) = ref_lstm(x_ref)\n",
    "# out_my, (hn_my, cn_my) = my_lstm(x)\n",
    "# print(\"out my: \", out_my)\n",
    "# print(\"out ref:\", out_ref)\n",
    "print(\"out_my shape: \", out_my.shape)    #  [NS, B,T,NH,D]\n",
    "print(\"out_ref shape: \", out_ref.shape)  # [B,T,H]\n",
    "# # Backward\n",
    "# loss_my = out_my.sum()\n",
    "# loss_ref = out_ref.sum()\n",
    "# loss_my.backward()\n",
    "# loss_ref.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 32])\n",
      "tensor([[[ 1.5234e+00,  2.0996e-01,  8.3984e-01,  1.8594e+00,  1.8125e+00,\n",
      "           8.6426e-02,  1.0469e+00,  1.7891e+00,  1.1484e+00,  1.7656e+00,\n",
      "           1.3281e+00,  8.7891e-01,  1.0000e+00,  1.8047e+00,  3.6133e-01,\n",
      "           1.1484e+00,  8.6328e-01,  6.0156e-01,  6.2109e-01,  9.8047e-01,\n",
      "           1.7344e+00,  1.1875e+00,  1.0469e+00,  1.6641e+00,  1.5234e+00,\n",
      "          -7.2266e-01,  1.8672e+00, -1.2598e-01,  1.4844e+00,  1.7031e+00,\n",
      "           1.1406e+00,  7.9297e-01],\n",
      "         [ 1.5000e+00,  7.9102e-02,  8.9062e-01,  1.9297e+00,  1.8672e+00,\n",
      "           1.9531e-01,  1.0078e+00,  1.8359e+00,  1.1328e+00,  1.8281e+00,\n",
      "           1.3125e+00,  9.4141e-01,  9.9219e-01,  1.8594e+00,  2.5781e-01,\n",
      "           1.0391e+00,  9.3359e-01,  5.9766e-01,  4.9023e-01,  9.7266e-01,\n",
      "           1.7734e+00,  1.1328e+00,  1.0469e+00,  1.7031e+00,  1.5156e+00,\n",
      "          -7.1094e-01,  1.9375e+00, -5.5176e-02,  1.4609e+00,  1.7188e+00,\n",
      "           1.0938e+00,  7.0312e-01],\n",
      "         [ 1.8125e+00,  1.3203e+00,  9.9609e-01,  1.9609e+00,  1.9531e+00,\n",
      "           3.0029e-02,  1.2656e+00,  1.9609e+00,  1.5469e+00,  1.9375e+00,\n",
      "           1.7188e+00,  9.9609e-01,  1.0391e+00,  1.9609e+00,  7.6172e-01,\n",
      "           1.7109e+00,  9.6875e-01,  8.6328e-01,  1.1016e+00,  1.0156e+00,\n",
      "           1.9219e+00,  1.6562e+00,  1.3828e+00,  1.9219e+00,  1.8438e+00,\n",
      "          -6.6406e-01,  1.9609e+00, -3.4912e-02,  1.8125e+00,  1.9453e+00,\n",
      "           1.4062e+00,  9.8438e-01],\n",
      "         [ 1.4297e+00,  3.7079e-03,  8.0469e-01,  1.8438e+00,  1.7734e+00,\n",
      "           1.4941e-01,  9.8047e-01,  1.7031e+00,  1.0469e+00,  1.7188e+00,\n",
      "           1.2188e+00,  8.6328e-01,  9.8828e-01,  1.7344e+00,  2.6953e-01,\n",
      "           9.1016e-01,  8.5156e-01,  5.2344e-01,  4.9023e-01,  9.5703e-01,\n",
      "           1.6797e+00,  1.0312e+00,  9.6875e-01,  1.5469e+00,  1.4062e+00,\n",
      "          -6.6797e-01,  1.8594e+00, -1.2988e-01,  1.3750e+00,  1.5547e+00,\n",
      "           1.0703e+00,  6.0547e-01],\n",
      "         [ 1.7969e+00,  1.3984e+00,  9.8047e-01,  1.9375e+00,  1.9219e+00,\n",
      "           8.5831e-04,  1.2969e+00,  1.9297e+00,  1.5547e+00,  1.9141e+00,\n",
      "           1.7109e+00,  9.8047e-01,  1.0547e+00,  1.9297e+00,  8.1250e-01,\n",
      "           1.6953e+00,  9.4141e-01,  8.4766e-01,  1.1484e+00,  1.0234e+00,\n",
      "           1.8984e+00,  1.6484e+00,  1.3906e+00,  1.8906e+00,  1.8203e+00,\n",
      "          -5.6641e-01,  1.9375e+00, -6.2012e-02,  1.7969e+00,  1.9219e+00,\n",
      "           1.4297e+00,  9.8828e-01],\n",
      "         [ 1.3828e+00,  8.7500e-01,  7.0703e-01,  1.5000e+00,  1.4844e+00,\n",
      "          -2.9102e-01,  1.1406e+00,  1.4688e+00,  1.0781e+00,  1.4297e+00,\n",
      "           1.2031e+00,  7.0703e-01,  1.0703e+00,  1.4609e+00,  7.2266e-01,\n",
      "           1.1406e+00,  6.1328e-01,  4.7070e-01,  9.4141e-01,  1.0234e+00,\n",
      "           1.4531e+00,  1.1250e+00,  1.0078e+00,  1.3359e+00,  1.3281e+00,\n",
      "          -1.3379e-01,  1.5234e+00, -4.1602e-01,  1.3203e+00,  1.4062e+00,\n",
      "           1.2109e+00,  8.9453e-01],\n",
      "         [ 1.7188e+00,  1.0234e+00,  9.3750e-01,  1.9062e+00,  1.8906e+00,\n",
      "           9.4604e-03,  1.2109e+00,  1.8984e+00,  1.4297e+00,  1.8750e+00,\n",
      "           1.6016e+00,  9.4531e-01,  1.0391e+00,  1.9062e+00,  6.5234e-01,\n",
      "           1.5703e+00,  9.1406e-01,  7.7734e-01,  9.8047e-01,  1.0156e+00,\n",
      "           1.8516e+00,  1.5234e+00,  1.2734e+00,  1.8359e+00,  1.7500e+00,\n",
      "          -6.9922e-01,  1.9141e+00, -8.5938e-02,  1.7109e+00,  1.8750e+00,\n",
      "           1.3281e+00,  9.6875e-01],\n",
      "         [ 1.5000e+00,  8.9844e-01,  7.6172e-01,  1.6562e+00,  1.6328e+00,\n",
      "          -1.8164e-01,  1.1797e+00,  1.6328e+00,  1.2031e+00,  1.5938e+00,\n",
      "           1.3359e+00,  7.6953e-01,  1.0625e+00,  1.6328e+00,  6.8750e-01,\n",
      "           1.2891e+00,  6.9922e-01,  5.5859e-01,  9.5312e-01,  1.0234e+00,\n",
      "           1.5938e+00,  1.2578e+00,  1.1016e+00,  1.5234e+00,  1.4766e+00,\n",
      "          -3.9648e-01,  1.6719e+00, -3.1445e-01,  1.4609e+00,  1.5859e+00,\n",
      "           1.2656e+00,  9.2188e-01]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cn_my=cn_my.reshape(num_layers, batch, hidden_size)\n",
    "print(cn_my.shape)\n",
    "\n",
    "print(cn_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 32])\n",
      "tensor([[[ 1.5234e+00,  2.0996e-01,  8.3984e-01,  1.8594e+00,  1.8125e+00,\n",
      "           8.6426e-02,  1.0469e+00,  1.7891e+00,  1.1484e+00,  1.7656e+00,\n",
      "           1.3281e+00,  8.7891e-01,  1.0000e+00,  1.8047e+00,  3.6133e-01,\n",
      "           1.1484e+00,  8.6328e-01,  6.0156e-01,  6.2109e-01,  9.8047e-01,\n",
      "           1.7344e+00,  1.1875e+00,  1.0469e+00,  1.6641e+00,  1.5234e+00,\n",
      "          -7.2266e-01,  1.8672e+00, -1.2598e-01,  1.4844e+00,  1.7031e+00,\n",
      "           1.1406e+00,  7.9297e-01],\n",
      "         [ 1.5000e+00,  7.9102e-02,  8.9062e-01,  1.9297e+00,  1.8672e+00,\n",
      "           1.9531e-01,  1.0078e+00,  1.8359e+00,  1.1328e+00,  1.8281e+00,\n",
      "           1.3125e+00,  9.4141e-01,  9.9219e-01,  1.8594e+00,  2.5781e-01,\n",
      "           1.0391e+00,  9.3359e-01,  5.9766e-01,  4.9023e-01,  9.7266e-01,\n",
      "           1.7734e+00,  1.1328e+00,  1.0469e+00,  1.7031e+00,  1.5156e+00,\n",
      "          -7.1094e-01,  1.9375e+00, -5.5176e-02,  1.4609e+00,  1.7188e+00,\n",
      "           1.0938e+00,  7.0312e-01],\n",
      "         [ 1.8125e+00,  1.3203e+00,  9.9609e-01,  1.9609e+00,  1.9531e+00,\n",
      "           3.0029e-02,  1.2656e+00,  1.9609e+00,  1.5469e+00,  1.9375e+00,\n",
      "           1.7188e+00,  9.9609e-01,  1.0391e+00,  1.9609e+00,  7.6172e-01,\n",
      "           1.7109e+00,  9.6875e-01,  8.6328e-01,  1.1016e+00,  1.0156e+00,\n",
      "           1.9219e+00,  1.6562e+00,  1.3828e+00,  1.9219e+00,  1.8438e+00,\n",
      "          -6.6406e-01,  1.9609e+00, -3.4912e-02,  1.8125e+00,  1.9453e+00,\n",
      "           1.4062e+00,  9.8438e-01],\n",
      "         [ 1.4297e+00,  1.0824e-04,  8.0078e-01,  1.8438e+00,  1.7734e+00,\n",
      "           1.4551e-01,  9.8047e-01,  1.6953e+00,  1.0391e+00,  1.7188e+00,\n",
      "           1.2188e+00,  8.5938e-01,  9.8828e-01,  1.7266e+00,  2.6758e-01,\n",
      "           9.1016e-01,  8.4766e-01,  5.1953e-01,  4.8828e-01,  9.5703e-01,\n",
      "           1.6719e+00,  1.0312e+00,  9.6484e-01,  1.5391e+00,  1.4062e+00,\n",
      "          -6.6797e-01,  1.8594e+00, -1.3379e-01,  1.3672e+00,  1.5469e+00,\n",
      "           1.0703e+00,  6.0547e-01],\n",
      "         [ 1.7969e+00,  1.3984e+00,  9.8047e-01,  1.9375e+00,  1.9219e+00,\n",
      "           8.5831e-04,  1.2969e+00,  1.9297e+00,  1.5547e+00,  1.9141e+00,\n",
      "           1.7109e+00,  9.8047e-01,  1.0547e+00,  1.9297e+00,  8.1250e-01,\n",
      "           1.6953e+00,  9.4141e-01,  8.4766e-01,  1.1484e+00,  1.0234e+00,\n",
      "           1.8984e+00,  1.6484e+00,  1.3906e+00,  1.8906e+00,  1.8203e+00,\n",
      "          -5.6641e-01,  1.9375e+00, -6.2012e-02,  1.7969e+00,  1.9219e+00,\n",
      "           1.4297e+00,  9.8828e-01],\n",
      "         [ 1.3828e+00,  8.7500e-01,  7.0703e-01,  1.5000e+00,  1.4844e+00,\n",
      "          -2.9102e-01,  1.1406e+00,  1.4688e+00,  1.0781e+00,  1.4297e+00,\n",
      "           1.2031e+00,  7.0703e-01,  1.0703e+00,  1.4609e+00,  7.2266e-01,\n",
      "           1.1406e+00,  6.1328e-01,  4.7070e-01,  9.4141e-01,  1.0234e+00,\n",
      "           1.4531e+00,  1.1250e+00,  1.0078e+00,  1.3359e+00,  1.3281e+00,\n",
      "          -1.3379e-01,  1.5234e+00, -4.1602e-01,  1.3203e+00,  1.4062e+00,\n",
      "           1.2109e+00,  8.9453e-01],\n",
      "         [ 1.7188e+00,  1.0156e+00,  9.3750e-01,  1.9062e+00,  1.8906e+00,\n",
      "           9.7656e-03,  1.2109e+00,  1.8984e+00,  1.4297e+00,  1.8750e+00,\n",
      "           1.6016e+00,  9.4531e-01,  1.0391e+00,  1.8984e+00,  6.5234e-01,\n",
      "           1.5703e+00,  9.1406e-01,  7.7734e-01,  9.7656e-01,  1.0156e+00,\n",
      "           1.8516e+00,  1.5234e+00,  1.2734e+00,  1.8359e+00,  1.7500e+00,\n",
      "          -6.9922e-01,  1.9141e+00, -8.5938e-02,  1.7109e+00,  1.8750e+00,\n",
      "           1.3281e+00,  9.6484e-01],\n",
      "         [ 1.5000e+00,  8.9453e-01,  7.6172e-01,  1.6562e+00,  1.6328e+00,\n",
      "          -1.8164e-01,  1.1797e+00,  1.6328e+00,  1.2031e+00,  1.5938e+00,\n",
      "           1.3359e+00,  7.6953e-01,  1.0625e+00,  1.6328e+00,  6.8750e-01,\n",
      "           1.2812e+00,  6.9922e-01,  5.5859e-01,  9.5312e-01,  1.0234e+00,\n",
      "           1.5938e+00,  1.2500e+00,  1.1016e+00,  1.5234e+00,  1.4688e+00,\n",
      "          -3.9648e-01,  1.6719e+00, -3.1445e-01,  1.4609e+00,  1.5859e+00,\n",
      "           1.2656e+00,  9.2188e-01]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(cn_ref.shape)\n",
    "\n",
    "print(cn_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5859375, -0.58984375, 1.828125, 1.703125, 1.515625, 1.1875, 1.890625, 1.9609375]\n",
      "[1.5859375, -0.58984375, 1.828125, 1.703125, 1.515625, 1.1875, 1.890625, 1.9609375]\n"
     ]
    }
   ],
   "source": [
    "result1 = cn_my[0, :, 0].tolist()  # list of 16 elements\n",
    "result2 = cn_ref[0, :, 0].tolist()  # list of 16 elements\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Forward] Output max abs diff: 0.00e+00\n",
      "[Forward] hn     max abs diff: 0.00e+00\n",
      "[Forward] cn     max abs diff: 0.00e+00\n",
      "[Forward] Output mean abs diff: 0.00e+00\n",
      "[Forward] hn     mean abs diff: 0.00e+00\n",
      "[Forward] cn     mean abs diff: 0.00e+00\n",
      "[Forward] Output max ref diff: 0.00e+00\n",
      "[Forward] hn     max ref diff: 0.00e+00\n",
      "[Forward] cn     max ref diff: 0.00e+00\n",
      "[Forward] Output mean ref diff: 0.00e+00\n",
      "[Forward] hn     mean ref diff: 0.00e+00\n",
      "[Forward] cn     mean ref diff: 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Output comparison\n",
    "print(f\"[Forward] Output max abs diff: {max_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max abs diff: {max_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     max abs diff: {max_abs_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean abs diff: {mean_abs_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean abs diff: {mean_abs_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     mean abs diff: {mean_abs_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output max ref diff: {max_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     max ref diff: {max_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     max ref diff: {max_ref_diff(cn_my, cn_ref):.2e}\")\n",
    "print(f\"[Forward] Output mean ref diff: {mean_ref_diff(out_my, out_ref):.2e}\")\n",
    "print(f\"[Forward] hn     mean ref diff: {mean_ref_diff(hn_my, hn_ref):.2e}\")\n",
    "print(f\"[Forward] cn     mean ref diff: {mean_ref_diff(cn_my, cn_ref):.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Gradients\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Grad] Input x     grad diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmax_abs_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mx_ref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Grad] Input x     grad diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_abs_diff(x.grad,\u001b[38;5;250m \u001b[39mx_ref.grad)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Grad] Input x     grad diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_ref_diff(x.grad,\u001b[38;5;250m \u001b[39mx_ref.grad)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mmax_abs_diff\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmax_abs_diff\u001b[39m(a, b):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m).abs().max().item()\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gradients\n",
    "print(f\"[Grad] Input x     grad diff: {max_abs_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {mean_abs_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {max_ref_diff(x.grad, x_ref.grad):.2e}\")\n",
    "print(f\"[Grad] Input x     grad diff: {mean_ref_diff(x.grad, x_ref.grad):.2e}\")\n",
    "\n",
    "print(f\"[Grad] h0          grad diff: {max_abs_diff(h0.grad, h0_ref.grad):.2e}\")\n",
    "print(f\"[Grad] c0          grad diff: {max_abs_diff(c0.grad, c0_ref.grad):.2e}\")\n",
    "\n",
    "for (n1, p1), (n2, p2) in zip(\n",
    "    my_lstm.named_parameters(), ref_lstm.named_parameters()\n",
    "):\n",
    "    if p1.grad is not None and p2.grad is not None:\n",
    "        diff = max_abs_diff(p1.grad, p2.grad)\n",
    "        print(f\"[Grad] Param {n1:20s} grad diff: {diff:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flashrnn)",
   "language": "python",
   "name": "flashrnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
